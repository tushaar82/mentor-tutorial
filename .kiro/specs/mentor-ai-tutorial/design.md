# Design Document

## Overview

This design document outlines the structure and content of a 10-day LLM-powered implementation guide for the Mentor AI EdTech platform. The guide enables two developers (Tushar - Backend, Vaishnavi - Frontend) to build the complete platform where 90% of code is generated by LLMs. The design ensures complete independence between backend and frontend work, allowing developers to work asynchronously on different schedules without blocking each other.

## How to Use This Documentation

### For First-Time Users

#### Step 1: Understand the Structure
The tutorial is organized into **task-based folders**, not traditional day-by-day guides. Each task is self-contained and can be completed independently.

```
tushar-backend/
â”œâ”€â”€ day-01-project-setup/          â† Start here
â”œâ”€â”€ day-02-firebase-authentication/ â† Then this
â”œâ”€â”€ day-03-user-onboarding-api/    â† Then this
...
```

#### Step 2: Read the README First
Every task folder starts with a `README.md` that explains:
- **What** you're building
- **Why** it matters
- **How** it works
- **Time estimate** (usually 1.5-2 hours)
- **Prerequisites** (what must be done first)

**Example workflow**:
```
1. Open: tushar-backend/day-01-project-setup/README.md
2. Read: What, Why, How sections
3. Check: Prerequisites (none for day 1)
4. Note: Time estimate (2 hours)
5. Proceed to: PROMPTS.md
```

#### Step 3: Follow the Document Order
Each task folder has documents in a specific order:

```
1. README.md          â† Read first (overview)
2. PROMPTS.md         â† Use second (generate code)
3. CONFIGURATION.md   â† Use third (manual setup)
4. TESTING.md         â† Use fourth (verify it works)
5. EXPECTED-OUTCOME.md â† Check fifth (success criteria)
6. TROUBLESHOOTING.md â† Use if needed (when stuck)
```

**Special documents** (only in some tasks):
- `USER-FLOW.md` - How users interact with this feature
- `AI-INTEGRATION.md` - Detailed AI/ML explanations
- `RAG-DEVELOPMENT.md` - Step-by-step RAG building

#### Step 4: Use AI Coding Agent Prompts Correctly

**In PROMPTS.md**, you'll find prompts optimized for AI coding agents (Windsurf, Copilot, Cursor).

**Two types of prompts**:

1. **Inline Prompts** (for Windsurf/Copilot in IDE)
2. **Chat Prompts** (for ChatGPT/Claude)

**Example Inline Prompt** (Windsurf/Copilot):
```markdown
## Prompt 1: Create FastAPI Project Structure

### For Windsurf/Copilot (Inline)

**Step 1**: Create file `backend/main.py`

**Step 2**: In the file, type this comment and press Tab/Enter:
```python
# Create a production-ready FastAPI application with:
# - CORS middleware for frontend (localhost:3000)
# - Error handling middleware
# - Health check endpoint at /health
# - API documentation at /docs
# - Proper logging configuration
# - Environment variable support
```

**Step 3**: Let Copilot generate the code

**Step 4**: Review and accept the suggestion

**Expected Code**:
```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import logging

app = FastAPI(title="Mentor AI Backend")

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    return {"status": "healthy"}
```
```

**Example Chat Prompt** (ChatGPT/Claude):
```markdown
## Prompt 1: Create FastAPI Project Structure

### For ChatGPT/Claude (Chat)

**Copy this entire prompt**:
```
Create a production-ready FastAPI project structure for a Python-based EdTech platform.

Project Name: mentor-ai-backend

Requirements:
1. Create folder structure with:
   - /services (business logic)
   - /routers (API endpoints)
   - /models (Pydantic models)
   - /utils (helper functions)
   - /tests (test files)

2. Generate main.py with:
   - FastAPI app initialization
   - CORS middleware for frontend (localhost:3000)
   - Error handling middleware
   - Health check endpoint
   - API documentation at /docs

3. Generate requirements.txt with:
   - fastapi==0.104.1
   - uvicorn[standard]==0.24.0
   - firebase-admin==6.2.0
   - python-dotenv==1.0.0
   - pydantic[email]==2.4.2

4. Generate .env.example with placeholder variables
5. Generate .gitignore for Python projects
6. Add comprehensive comments explaining each part

Provide:
- Complete folder structure (show tree)
- Full code for each file
- Instructions for setup
```

**What You'll Get**: Complete project structure with all files

**What to Do**:
1. Copy the generated code
2. Create files at specified paths
3. Save
```

**How to use with Windsurf/Copilot**:

1. **Open your IDE** (VS Code with Copilot or Windsurf)
2. **Create the file** mentioned in the prompt
3. **Type the comment** from the inline prompt
4. **Press Tab** to trigger Copilot
5. **Review** the generated code
6. **Accept** if correct, or **modify** the comment and try again
7. **Save** the file

**How to use with ChatGPT/Claude**:

1. **Copy** the entire chat prompt
2. **Paste** into ChatGPT or Claude
3. **Wait** for code generation
4. **Copy** the generated code
5. **Create** the file at the specified path
6. **Paste** and save

**Pro tips**:
- **Windsurf/Copilot**: Be specific in comments, use natural language
- **ChatGPT/Claude**: Use structured prompts with clear requirements
- **Both**: Review generated code before accepting
- **Both**: Test immediately after generation

#### Step 5: Complete Configuration Steps

**In CONFIGURATION.md**, you'll find manual setup steps:

```markdown
## Step 1: Install Python

### What You're Doing
Installing Python 3.11 on your system.

### Command/Action
```bash
# On Ubuntu/Debian
sudo apt install python3.11

# On macOS
brew install python@3.11
```

### Verification
```bash
python3.11 --version
# Should show: Python 3.11.x
```
```

**How to use**:
1. **Read** what you're doing and why
2. **Copy** the command
3. **Paste** into your terminal
4. **Run** the command
5. **Verify** it worked with the verification command

#### Step 6: Test Your Work

**In TESTING.md**, you'll find tests like this:

```markdown
## Test 1: Verify API Starts

### Steps
1. Run: `uvicorn main:app --reload`
2. Open browser: `http://localhost:8000`
3. You should see: `{"message": "Welcome to Mentor AI"}`

### Expected Result
```json
{"message": "Welcome to Mentor AI"}
```

### If It Fails
- Check: Python version is 3.11+
- Check: All dependencies installed
- Fix: Run `pip install -r requirements.txt`
```

**How to use**:
1. **Follow** each step exactly
2. **Compare** your result with expected result
3. **If it matches**: âœ“ Test passed, continue
4. **If it doesn't match**: Check "If It Fails" section

#### Step 7: Verify Success

**In EXPECTED-OUTCOME.md**, check off items:

```markdown
## Success Checklist
- [ ] All prompts executed successfully
- [ ] All configuration steps completed
- [ ] All tests passing
- [ ] No errors in logs
- [ ] Ready to move to next task
```

**Only proceed to the next task when ALL items are checked!**

### For Tushar (Backend Developer)

#### Your Daily Workflow

**Morning** (Start a new task):
1. Open `tushar-backend/day-XX-task-name/`
2. Read `README.md` completely
3. Check prerequisites are met
4. Open `PROMPTS.md`

**Mid-Morning** (Generate code):
5. Copy first prompt to LLM (ChatGPT/Claude)
6. Copy generated code to specified file
7. Repeat for all prompts in the task
8. Open `CONFIGURATION.md`

**Afternoon** (Configure and test):
9. Complete all configuration steps
10. Verify each step worked
11. Open `TESTING.md`
12. Run all tests
13. Verify all tests pass

**End of Day** (Verify and commit):
14. Open `EXPECTED-OUTCOME.md`
15. Check all items in success checklist
16. Commit code to Git
17. Mark task as complete

#### Working Independently

You don't need Vaishnavi's frontend to test your backend:

**Use curl for API testing**:
```bash
# Test registration endpoint
curl -X POST http://localhost:8000/api/auth/register/parent \
  -H "Content-Type: application/json" \
  -d '{"email": "test@example.com", "language": "en"}'
```

**Use Python test scripts**:
```bash
# Run automated tests
python mock-data/test-scripts.py
```

**Use Firebase Emulator**:
```bash
# Test with local Firebase
firebase emulators:start
```

### For Vaishnavi (Frontend Developer)

#### Your Daily Workflow

**Morning** (Start a new task):
1. Open `vaishnavi-frontend/day-XX-task-name/`
2. Read `README.md` completely
3. Check prerequisites are met
4. Start mock API server: `node mock-data/mock-api-server.js`
5. Open `PROMPTS.md`

**Mid-Morning** (Generate code):
6. Copy first prompt to LLM (ChatGPT/Claude)
7. Copy generated code to specified file
8. Repeat for all prompts in the task
9. Open `CONFIGURATION.md`

**Afternoon** (Configure and test):
10. Complete all configuration steps
11. Start dev server: `npm run dev`
12. Open `TESTING.md`
13. Test in browser with mock data
14. Verify all tests pass

**End of Day** (Verify and commit):
15. Open `EXPECTED-OUTCOME.md`
16. Check all items in success checklist
17. Take screenshots of working UI
18. Commit code to Git
19. Mark task as complete

#### Working Independently

You don't need Tushar's backend to test your frontend:

**Use mock API server**:
```bash
# Start mock server (provides fake API responses)
node mock-data/mock-api-server.js
```

**Use mock data files**:
```javascript
// Import mock data in your components
import mockAnalytics from '../mock-data/mock-api-responses.json';
```

**Test UI without backend**:
```javascript
// Use mock data instead of API calls
const analytics = useMockData ? mockAnalytics : await fetchAnalytics();
```

### Understanding AI Integration Tasks

Some tasks (Day 4, 5, 7, 8) involve AI/ML. These have extra documentation:

#### AI-INTEGRATION.md
Explains the AI service in detail:
- What it is (Vector Search, Gemini, RAG)
- How it works (architecture diagrams)
- Code walkthrough (line-by-line explanations)
- Testing AI integration
- Common AI issues

**When to read**: Before starting the task, to understand the AI concepts.

#### RAG-DEVELOPMENT.md
Step-by-step guide to building RAG:
- What RAG is and why we use it
- RAG architecture
- Implementation steps
- Testing RAG
- Best practices

**When to read**: During Day 5 (RAG Implementation task).

### When You Get Stuck

#### Step 1: Check TROUBLESHOOTING.md
Every task has common issues and solutions:

```markdown
### Issue 1: API Returns 500 Error

**Symptoms**: Server crashes when calling endpoint

**Possible Causes**:
1. Missing environment variable
2. Firebase not initialized

**Solutions**:
1. Check `.env` file has all variables
2. Verify Firebase credentials are correct
```

#### Step 2: Check Logs
```bash
# Backend logs
tail -f logs/app.log

# Frontend logs
# Check browser console (F12)
```

#### Step 3: Test Individual Components
```bash
# Test just the database connection
python -c "from services.db import connect; connect()"

# Test just the API endpoint
curl http://localhost:8000/health
```

#### Step 4: Ask for Help
If still stuck:
1. Note which task you're on
2. Note which step failed
3. Copy the error message
4. Check the error in TROUBLESHOOTING.md
5. If not found, ask team/community with:
   - Task name
   - Step number
   - Error message
   - What you've tried

### Integration Testing (Optional)

When both Tushar and Vaishnavi are ready:

1. **Stop mock servers**
2. **Connect frontend to real backend**:
   ```javascript
   // Update API URL in frontend
   const API_URL = 'http://localhost:8000';
   ```
3. **Open** `integration/INTEGRATION-GUIDE.md`
4. **Follow** end-to-end test scenarios
5. **Verify** complete user flows work

### Progress Tracking

#### Daily Checklist
```markdown
## Day X Progress

- [ ] Read README.md
- [ ] Generated all code from PROMPTS.md
- [ ] Completed all CONFIGURATION.md steps
- [ ] All tests in TESTING.md passing
- [ ] Verified EXPECTED-OUTCOME.md checklist
- [ ] Code committed to Git
- [ ] Ready for next task
```

#### Weekly Review
Every 2-3 days, review:
- What tasks completed
- What challenges faced
- What learned
- What to improve

### Tips for Success

#### 1. Don't Skip Steps
Complete tasks in order. Each task builds on previous ones.

#### 2. Test Frequently
Don't wait until the end. Test after each prompt.

#### 3. Read Error Messages
Error messages tell you exactly what's wrong. Read them carefully.

#### 4. Use Version Control
Commit after each completed task:
```bash
git add .
git commit -m "Completed day-XX-task-name"
```

#### 5. Take Breaks
Each task is 1.5-2 hours. Take a 10-minute break between tasks.

#### 6. Ask Questions
If something is unclear, ask before proceeding.

#### 7. Document Issues
If you find a bug or unclear instruction, note it down.

### Common Mistakes to Avoid

#### âŒ Modifying LLM Prompts
**Don't**: Change the prompts to "improve" them
**Do**: Use prompts exactly as written

#### âŒ Skipping Configuration Steps
**Don't**: Skip steps thinking they're optional
**Do**: Complete every configuration step

#### âŒ Not Testing
**Don't**: Generate all code then test at the end
**Do**: Test after each prompt

#### âŒ Proceeding with Failures
**Don't**: Move to next task when tests fail
**Do**: Fix all issues before proceeding

#### âŒ Working Without Mock Data
**Don't**: Wait for the other developer
**Do**: Use mock data to work independently

### Quick Reference

#### File Structure
```
task-folder/
â”œâ”€â”€ README.md          â†’ Read first
â”œâ”€â”€ PROMPTS.md         â†’ Generate code
â”œâ”€â”€ CONFIGURATION.md   â†’ Manual setup
â”œâ”€â”€ TESTING.md         â†’ Verify it works
â”œâ”€â”€ EXPECTED-OUTCOME.md â†’ Success criteria
â”œâ”€â”€ USER-FLOW.md       â†’ User interaction (some tasks)
â”œâ”€â”€ AI-INTEGRATION.md  â†’ AI details (AI tasks)
â”œâ”€â”€ RAG-DEVELOPMENT.md â†’ RAG guide (RAG tasks)
â””â”€â”€ TROUBLESHOOTING.md â†’ When stuck
```

#### Command Quick Reference
```bash
# Backend (Tushar)
uvicorn main:app --reload          # Start API
pytest tests/                      # Run tests
python mock-data/test-scripts.py   # Test with scripts

# Frontend (Vaishnavi)
npm run dev                        # Start dev server
node mock-data/mock-api-server.js  # Start mock API
npm test                           # Run tests

# Both
git add . && git commit -m "msg"   # Commit code
```

### Using AI Coding Agents Effectively

#### Windsurf Setup

**Installation**:
1. Download Windsurf from [windsurf.ai](https://windsurf.ai)
2. Install and open
3. Sign in with your account
4. Open your project folder

**Best Practices**:
- **Be specific**: "Create FastAPI endpoint for parent registration with email, phone, and Google OAuth"
- **Use context**: Windsurf sees your entire project, reference other files
- **Iterate**: If first generation isn't perfect, refine your comment
- **Review**: Always review generated code before accepting

**Example Workflow**:
```python
# Step 1: Create file backend/services/auth_service.py

# Step 2: Type this comment
# Create authentication service for parent registration
# Support: email, phone, Google OAuth
# Use Firebase Admin SDK
# Return: parent_id, verification_required

# Step 3: Windsurf generates code

# Step 4: If incomplete, add:
# Add function to verify email with code
# Input: parent_id, verification_code
# Output: verified boolean

# Step 5: Windsurf adds the function
```

#### GitHub Copilot Setup

**Installation**:
1. Install VS Code
2. Install GitHub Copilot extension
3. Sign in with GitHub account
4. Enable Copilot

**Best Practices**:
- **Comment-driven**: Write detailed comments, Copilot generates code
- **Function signatures**: Write function signature, Copilot fills body
- **Tab to accept**: Press Tab to accept suggestion
- **Alt+]**: Cycle through alternative suggestions

**Example Workflow**:
```python
# Step 1: Create file backend/services/auth_service.py

# Step 2: Write function signature
def register_parent(email: str, phone: str, language: str) -> dict:
    """
    Register a new parent account.
    
    Args:
        email: Parent email address
        phone: Parent phone number
        language: Preferred language (en, hi, mr)
    
    Returns:
        dict with parent_id and verification_required
    """
    # Press Tab here, Copilot generates function body

# Step 3: Review and accept
```

#### ChatGPT/Claude Setup

**When to Use**:
- Complex logic that needs explanation
- Multiple files at once
- Architecture decisions
- Debugging help

**Best Practices**:
- **Structured prompts**: Use the chat prompt format from PROMPTS.md
- **Iterate**: If output isn't perfect, ask for modifications
- **Copy carefully**: Ensure you copy all code, including imports
- **Test immediately**: Test generated code right away

**Example Workflow**:
1. Copy prompt from PROMPTS.md
2. Paste into ChatGPT/Claude
3. Review generated code
4. Copy code to your IDE
5. Create file and paste
6. Test immediately

#### Choosing the Right Tool

| Task | Best Tool | Why |
|------|-----------|-----|
| Single file generation | Windsurf/Copilot | Faster, in-IDE |
| Multiple related files | ChatGPT/Claude | Better context across files |
| Complex algorithms | ChatGPT/Claude | Better at explaining logic |
| Boilerplate code | Windsurf/Copilot | Faster for repetitive code |
| Debugging | Windsurf/Copilot | Sees your entire codebase |
| Architecture design | ChatGPT/Claude | Better at high-level thinking |

#### Hybrid Approach (Recommended)

**Use both tools together**:

1. **ChatGPT/Claude** for architecture and design
   - "Design the authentication service architecture"
   - Get high-level structure and approach

2. **Windsurf/Copilot** for implementation
   - Use the architecture from ChatGPT
   - Generate actual code in IDE
   - Faster iteration

3. **ChatGPT/Claude** for debugging
   - Paste error messages
   - Get explanation and fix

**Example**:
```
Day 2: Firebase Authentication

1. Ask ChatGPT: "Design authentication service for parent registration"
2. Get architecture and function list
3. Open Windsurf, create auth_service.py
4. Use Copilot to generate each function based on architecture
5. If error, paste error to ChatGPT for help
```

#### AI Agent Limitations

**What AI agents are good at**:
- âœ… Generating boilerplate code
- âœ… Implementing well-defined functions
- âœ… Following patterns from your codebase
- âœ… Writing tests
- âœ… Fixing syntax errors

**What AI agents struggle with**:
- âŒ Understanding your specific business logic
- âŒ Making architectural decisions
- âŒ Knowing your exact requirements
- âŒ Testing edge cases
- âŒ Security best practices

**Your responsibility**:
- Review all generated code
- Test thoroughly
- Ensure security
- Verify business logic
- Handle edge cases

### Getting Started

**Ready to begin?**

**Step 1: Choose Your AI Tool**
- **Windsurf**: Download and install
- **GitHub Copilot**: Install VS Code extension
- **ChatGPT/Claude**: Create account

**Step 2: Open Tutorial**
1. **Tushar**: Open `tushar-backend/day-01-project-setup/README.md`
2. **Vaishnavi**: Open `vaishnavi-frontend/day-01-project-setup/README.md`

**Step 3: Follow Workflow**
1. Read README.md
2. Open PROMPTS.md
3. Choose inline (Windsurf/Copilot) or chat (ChatGPT/Claude) prompt
4. Generate code
5. Test immediately
6. Move to next prompt

**Step 4: Complete Task**
1. All prompts executed
2. All tests passing
3. Code committed
4. Move to next task

**Good luck! You've got this! ðŸš€**

## Architecture

### Tutorial Structure (Extended to 10 Days)

```
mentor-ai-tutorial/
â”œâ”€â”€ README.md (How to use this guide)
â”œâ”€â”€ PREREQUISITES.md (Software installation)
â”œâ”€â”€ tushar-backend/
â”‚   â”œâ”€â”€ day-01-project-setup/
â”‚   â”‚   â”œâ”€â”€ README.md (Task overview and learning objectives)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md (All LLM prompts for this task)
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md (Manual setup steps)
â”‚   â”‚   â”œâ”€â”€ TESTING.md (How to test this task)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (What you should have)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md (Common issues)
â”‚   â”œâ”€â”€ day-02-firebase-authentication/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Firebase Auth, Why: Secure user management, How: OAuth + Email/Phone)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md (Firebase Console setup)
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test with curl commands)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Working auth endpoints)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Parent registration â†’ Login flow)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-03-user-onboarding-api/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Onboarding endpoints, Why: Collect user data, How: Firestore integration)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Preferences â†’ Child profile â†’ Exam selection)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-04-vector-search-setup/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Vector Search, Why: Semantic syllabus search, How: Embeddings + Index)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md (Enable Vertex AI, create index)
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test embedding generation and queries)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Working vector search)
â”‚   â”‚   â”œâ”€â”€ AI-INTEGRATION.md (Detailed Vector Search explanation)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-05-rag-implementation/
â”‚   â”‚   â”œâ”€â”€ README.md (What: RAG, Why: Accurate questions, How: Retrieval + Generation)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test question generation with context)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Questions based on syllabus)
â”‚   â”‚   â”œâ”€â”€ AI-INTEGRATION.md (RAG flow diagram and explanation)
â”‚   â”‚   â”œâ”€â”€ RAG-DEVELOPMENT.md (Step-by-step RAG building)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-06-diagnostic-test-generation/
â”‚   â”‚   â”œâ”€â”€ README.md (What: 200-question test, Why: Assess student, How: RAG + Patterns)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Generate and validate test)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Complete diagnostic test)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Student starts test â†’ Answers â†’ Submits)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-07-gemini-analytics/
â”‚   â”‚   â”œâ”€â”€ README.md (What: AI analytics, Why: Identify weaknesses, How: Gemini Flash)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md (Enable Gemini API)
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test analytics generation)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Detailed analytics report)
â”‚   â”‚   â”œâ”€â”€ AI-INTEGRATION.md (Gemini Flash explanation)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Test submitted â†’ Analytics generated â†’ Displayed)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-08-schedule-generation/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Study schedule, Why: Personalized plan, How: Gemini + Weightages)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Generate schedule with different scenarios)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Day-by-day schedule)
â”‚   â”‚   â”œâ”€â”€ AI-INTEGRATION.md (Schedule generation logic)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Analytics â†’ Schedule â†’ Daily tasks)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-09-payment-integration/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Razorpay, Why: Subscriptions, How: Payment flow)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md (Razorpay account setup)
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test payment in sandbox)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Working payment flow)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Parent upgrades â†’ Payment â†’ Premium access)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-10-cloud-run-deployment/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Google Cloud Run, Why: Serverless production, How: Docker + Cloud Build)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md (Dockerfile, cloudbuild.yaml, deployment scripts)
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md (GCP project setup, Cloud Run service)
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test deployed API on Cloud Run)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Live backend API on Cloud Run URL)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Deploy â†’ Test â†’ Monitor)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ APPENDIX-backend.md
â”‚   â””â”€â”€ mock-data/
â”‚       â”œâ”€â”€ sample-requests.json
â”‚       â””â”€â”€ test-scripts.py
â”œâ”€â”€ vaishnavi-frontend/
â”‚   â”œâ”€â”€ day-01-project-setup/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-02-authentication-ui/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Login/Register UI, Why: User access, How: Firebase Auth)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test with mock backend)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Working auth forms)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (User sees form â†’ Enters data â†’ Authenticated)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-03-onboarding-flow/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Multi-step onboarding wizard)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-04-diagnostic-test-ui/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Test interface, Why: Student assessment, How: Question display + Timer)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test with mock questions)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Interactive test UI)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Start test â†’ Answer questions â†’ Submit)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-05-analytics-visualization/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Charts and graphs, Why: Show performance, How: Chart libraries)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test with mock analytics data)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Visual analytics dashboard)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (View analytics â†’ Understand weaknesses)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-06-schedule-display/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Schedule UI, Why: Show daily tasks, How: Calendar component)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test with mock schedule)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Interactive schedule)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (View schedule â†’ Mark complete â†’ Track progress)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-07-practice-module/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Practice UI, Why: Topic practice, How: Question display + Feedback)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Select topic â†’ Practice â†’ See results)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-08-parent-dashboard/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Parent view, Why: Monitor child, How: Progress display)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Parent logs in â†’ Views progress â†’ Gets insights)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-09-payment-ui/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Payment flow, Why: Subscription, How: Razorpay integration)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test with Razorpay sandbox)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Working payment UI)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Click upgrade â†’ Enter payment â†’ Premium activated)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ day-10-firebase-hosting-deployment/
â”‚   â”‚   â”œâ”€â”€ README.md (What: Firebase Hosting, Why: Fast CDN delivery, How: Firebase deploy)
â”‚   â”‚   â”œâ”€â”€ PROMPTS.md (Build config, deployment scripts)
â”‚   â”‚   â”œâ”€â”€ CONFIGURATION.md (Firebase Hosting setup, custom domain)
â”‚   â”‚   â”œâ”€â”€ TESTING.md (Test deployed frontend)
â”‚   â”‚   â”œâ”€â”€ EXPECTED-OUTCOME.md (Live frontend on Firebase Hosting URL)
â”‚   â”‚   â”œâ”€â”€ USER-FLOW.md (Build â†’ Deploy â†’ Verify)
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ APPENDIX-frontend.md
â”‚   â””â”€â”€ mock-data/
â”‚       â”œâ”€â”€ mock-api-responses.json
â”‚       â””â”€â”€ mock-api-server.js
â””â”€â”€ integration/
    â”œâ”€â”€ INTEGRATION-GUIDE.md
    â””â”€â”€ end-to-end-tests.md
```

### Task-Based Document Structure

Each task folder contains multiple focused markdown files:

#### README.md (Task Overview)
```markdown
# Task: [Task Name]

## What You're Building
[Clear description of the feature/component]

## Why This Matters
[Business value and user benefit]

## How It Works
[Technical approach and architecture]

## Learning Objectives
By completing this task, you will:
- Understand [concept 1]
- Learn how to [skill 1]
- Implement [technology 1]

## Time Estimate
- LLM Code Generation: 30 minutes
- Configuration: 30 minutes
- Testing: 30 minutes
- Total: 1.5-2 hours

## Prerequisites
- Completed: [Previous task]
- Installed: [Required software]
- Configured: [Required services]

## Files You'll Create
- `path/to/file1.py` - [Purpose]
- `path/to/file2.py` - [Purpose]

## Next Steps
After completing this task, move to: [Next task]
```

#### PROMPTS.md (All AI Coding Agent Prompts)
```markdown
# AI Coding Agent Prompts for [Task Name]

## Prompt 1: [Component Name]

### Purpose
[What this code does and why you need it]

### When to Use
[Before/after which step]

---

### Option A: For Windsurf/Copilot (Inline in IDE)

**Step 1**: Create file `path/to/file.py`

**Step 2**: Type this comment at the top of the file:
```python
# Create [component name] with the following:
# - [Feature 1]
# - [Feature 2]
# - [Feature 3]
# 
# Requirements:
# - [Requirement 1]
# - [Requirement 2]
# - Include error handling
# - Add type hints
# - Add docstrings
```

**Step 3**: Press Tab to trigger Copilot

**Step 4**: Review and accept the generated code

**Step 5**: If incomplete, add more specific comments:
```python
# Now add function to [specific functionality]
# Input: [describe input]
# Output: [describe output]
# Handle errors: [specific errors]
```

---

### Option B: For ChatGPT/Claude (Chat Interface)

**Copy this entire prompt**:
```
Create [component name] for [project context].

CONTEXT:
- Project: Mentor AI EdTech Platform
- Stack: [Specific tech for this prompt]
- File: path/to/file.py

GENERATE:
[Exactly what to create]

REQUIREMENTS:
1. [Specific requirement]
2. [Specific requirement]
3. Include error handling
4. Add TypeScript types / Python type hints
5. Add detailed comments
6. [Additional requirements]

INTEGRATE WITH:
- [Other file/service this connects to]
- [API endpoint or database it uses]

OUTPUT FORMAT:
- Provide complete code (no placeholders)
- Include all imports
- Add example usage if applicable

TESTING:
[How to test this code standalone]
```

---

### What You'll Get
- File: `path/to/file.py`
- Contains: [List of functions/classes]
- Dependencies: [List of imports]

### What to Do With It

**If using Windsurf/Copilot**:
1. Code is already in the file
2. Review for correctness
3. Save the file
4. Install dependencies: `pip install [packages]`

**If using ChatGPT/Claude**:
1. Copy the generated code
2. Create file: `path/to/file.py`
3. Paste the code
4. Save the file
5. Install dependencies: `pip install [packages]`

### Expected Code Structure
```python
# High-level structure (not full code)
class ServiceName:
    def __init__(self):
        # Initialize
        
    def main_function(self):
        # Main logic
```

### Verification
Run this to verify code was generated correctly:
```bash
python -c "from module import function; print('âœ“ Import successful')"
```

### Troubleshooting AI Generation

**If Copilot doesn't generate**:
- Make comment more specific
- Add example input/output
- Break into smaller comments

**If ChatGPT generates incomplete code**:
- Ask: "Continue the code from where you stopped"
- Ask: "Add the missing [specific part]"
- Regenerate with more specific requirements

---

[Repeat for each prompt in this task]
```

#### CONFIGURATION.md (Manual Setup)
```markdown
# Configuration Steps for [Task Name]

## Overview
These are the ONLY manual steps (no coding). Each step includes exact commands and verification.

## Step 1: [Action Name]

### What You're Doing
[Clear explanation]

### Why This Is Needed
[Reason]

### Command/Action
```bash
[Exact command to run]
```

### Expected Output
```
[What you should see]
```

### Verification
```bash
[Command to verify it worked]
```

### If It Fails
- **Issue**: [Common problem]
- **Cause**: [Why it happens]
- **Fix**: [How to resolve]

---

[Repeat for each configuration step]

## Environment Variables

Add these to your `.env` file:

```bash
VARIABLE_NAME=value  # Description - Get from [source]
ANOTHER_VAR=value    # Description - Get from [source]
```

## Verification Checklist
- [ ] Step 1 completed successfully
- [ ] Step 2 completed successfully
- [ ] All environment variables set
- [ ] Services are running
```

#### TESTING.md (Standalone Testing)
```markdown
# Testing Guide for [Task Name]

## Overview
Test your work independently without requiring other components.

## Test 1: [Test Name]

### What You're Testing
[Feature or function being tested]

### Prerequisites
- [Required setup]

### Steps
1. [Exact action]
2. [Exact action]
3. [Exact action]

### Expected Result
```
[Exact output or behavior]
```

### Actual Result
[Space for you to note what happened]

### If It Fails
**Symptom**: [What you see]
**Possible Causes**:
1. [Cause 1] â†’ Fix: [Solution 1]
2. [Cause 2] â†’ Fix: [Solution 2]

**Debug Commands**:
```bash
[Commands to diagnose the issue]
```

---

[Repeat for each test]

## Automated Test Script

Run all tests at once:

```bash
python test_[task_name].py
```

Expected output:
```
âœ“ Test 1: [Name] - PASSED
âœ“ Test 2: [Name] - PASSED
âœ“ Test 3: [Name] - PASSED

All tests passed! âœ“
```

## Test Data

Use this sample data for testing:

```json
{
  "test_input": {
    "field1": "value1",
    "field2": "value2"
  }
}
```
```

#### EXPECTED-OUTCOME.md (Success Criteria)
```markdown
# Expected Outcome for [Task Name]

## What You Should Have

### Files Created
- [ ] `path/to/file1.py` - [Purpose]
- [ ] `path/to/file2.py` - [Purpose]
- [ ] `.env` with required variables

### Services Running
- [ ] [Service name] running on port [number]
- [ ] [Database/API] accessible

### Functionality Working
- [ ] [Feature 1] works as expected
- [ ] [Feature 2] works as expected
- [ ] All tests passing

## Visual Verification

### For Backend
You should be able to:
```bash
curl http://localhost:8000/endpoint
```

And see:
```json
{
  "status": "success",
  "data": {...}
}
```

### For Frontend
You should see:
- [UI element 1] displayed correctly
- [UI element 2] functioning properly
- No console errors

## Screenshots/Examples

[Include example outputs, API responses, or UI screenshots]

## Success Checklist
- [ ] All prompts executed successfully
- [ ] All configuration steps completed
- [ ] All tests passing
- [ ] Expected output matches actual output
- [ ] No errors in logs
- [ ] Ready to move to next task

## If Something's Not Working

Don't proceed to the next task until:
1. All tests pass
2. Expected outcome matches actual outcome
3. You understand what the code does

Refer to TROUBLESHOOTING.md for help.
```

#### USER-FLOW.md (User Journey)
```markdown
# User Flow for [Task Name]

## Overview
This document explains how users interact with this feature.

## User Story
As a [user type], I want to [action], so that [benefit].

## Flow Diagram

```
[User Action 1]
      â†“
[System Response 1]
      â†“
[User Action 2]
      â†“
[System Response 2]
      â†“
[Final Outcome]
```

## Detailed Steps

### Step 1: [User Action]
**User does**: [Specific action]
**System does**: [Backend/Frontend response]
**User sees**: [Visual feedback]

### Step 2: [Next Action]
**User does**: [Specific action]
**System does**: [Backend/Frontend response]
**User sees**: [Visual feedback]

[Continue for all steps]

## Example Scenario

**Scenario**: Parent registers for the first time

1. **User**: Opens registration page
   - **Sees**: Registration form with email, phone, Google Sign-In options
   
2. **User**: Clicks "Sign in with Google"
   - **System**: Redirects to Google OAuth
   - **Sees**: Google account selection
   
3. **User**: Selects Google account
   - **System**: Verifies token, creates parent account
   - **Sees**: Success message, redirects to preferences
   
4. **User**: Selects language (Hindi)
   - **System**: Saves preference, updates UI to Hindi
   - **Sees**: Onboarding flow in Hindi

## Edge Cases

### Case 1: [Scenario]
**What happens**: [Description]
**System handles**: [How it's handled]
**User sees**: [Feedback]

## Success Criteria
User can successfully:
- [ ] [Complete action 1]
- [ ] [Complete action 2]
- [ ] [Reach desired outcome]
```

#### AI-INTEGRATION.md (For AI-related tasks)
```markdown
# AI Integration Details for [Task Name]

## Overview
This document explains the AI/ML components in detail.

## AI Service Used
- **Service**: [Vertex AI / Gemini Flash / Vector Search]
- **Model**: [Specific model name and version]
- **Purpose**: [What it does]

## How It Works

### Architecture
```
[Input Data]
      â†“
[Preprocessing]
      â†“
[AI Service Call]
      â†“
[Response Processing]
      â†“
[Output]
```

### Detailed Flow

1. **Input Preparation**
   - What data is needed
   - How it's formatted
   - Example input

2. **AI Service Call**
   - API endpoint used
   - Request format
   - Authentication

3. **Response Handling**
   - Expected response format
   - Parsing logic
   - Error handling

4. **Output Generation**
   - How response is used
   - Data transformation
   - Storage/display

## Code Walkthrough

### Key Function: [Function Name]
```python
def function_name(input_data):
    """
    What this function does.
    
    Args:
        input_data: Description
    
    Returns:
        Description
    """
    # Step 1: Prepare input
    prepared_data = prepare(input_data)
    
    # Step 2: Call AI service
    response = ai_service.call(prepared_data)
    
    # Step 3: Process response
    result = process(response)
    
    return result
```

**Line-by-line explanation**:
- Line X: [What it does and why]
- Line Y: [What it does and why]

## Prompt Engineering (for Gemini)

### Prompt Structure
```
[Context]
[Task Description]
[Requirements]
[Output Format]
[Examples]
```

### Example Prompt
```
You are an expert JEE exam question generator.

CONTEXT:
- Exam: JEE Main
- Topic: Mechanics
- Difficulty: Medium

TASK:
Generate 5 multiple-choice questions.

REQUIREMENTS:
1. Based on provided syllabus
2. Include 4 options
3. Provide explanations

OUTPUT FORMAT:
JSON array of questions

EXAMPLE:
[{
  "question": "...",
  "options": {...},
  "answer": "A",
  "explanation": "..."
}]
```

### Why This Prompt Works
- [Reason 1]
- [Reason 2]

## Testing AI Integration

### Test 1: [Test Name]
**Input**:
```json
{...}
```

**Expected AI Response**:
```json
{...}
```

**Verification**:
```bash
[Command to test]
```

## Common AI Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Empty response | Invalid input | Validate input format |
| Rate limit | Too many calls | Add retry logic |
| Invalid JSON | LLM formatting | Parse with regex |

## Performance Considerations

- **Latency**: [Expected response time]
- **Cost**: [API cost per call]
- **Rate Limits**: [Calls per minute]
- **Optimization**: [How to reduce costs/improve speed]

## Further Reading
- [Link to official docs]
- [Link to tutorials]
- [Link to examples]
```

#### RAG-DEVELOPMENT.md (For RAG tasks)
```markdown
# RAG Development Guide for [Task Name]

## What is RAG?

RAG (Retrieval-Augmented Generation) combines:
1. **Retrieval**: Finding relevant information
2. **Augmentation**: Adding that information to context
3. **Generation**: Creating new content based on context

## Why RAG for Mentor AI?

Without RAG:
- LLM generates questions from training data
- May not match current syllabus
- Can hallucinate incorrect information

With RAG:
- Questions based on actual syllabus
- Always up-to-date
- Factually accurate

## RAG Architecture

```
User Request: "Generate mechanics questions"
      â†“
[1. RETRIEVAL]
Query Vector Search for "mechanics" content
      â†“
Retrieved: "Newton's laws, friction, circular motion..."
      â†“
[2. AUGMENTATION]
Add retrieved content to LLM prompt
      â†“
Prompt: "Using this syllabus: [content], generate questions..."
      â†“
[3. GENERATION]
LLM generates questions based on syllabus
      â†“
Output: Accurate, syllabus-aligned questions
```

## Step-by-Step Implementation

### Step 1: Set Up Vector Search
[Link to vector search task]

### Step 2: Create Retrieval Function
```python
def retrieve_context(topic: str, exam_type: str) -> str:
    """
    Retrieve relevant syllabus content.
    """
    # Query vector search
    results = vector_search.query(topic, exam_type, top_k=5)
    
    # Combine results
    context = "\n\n".join([r["text"] for r in results])
    
    return context
```

**What this does**:
- Searches vector database for topic
- Returns top 5 most relevant chunks
- Combines into single context string

### Step 3: Create Augmented Prompt
```python
def create_prompt_with_context(topic: str, context: str) -> str:
    """
    Create LLM prompt with retrieved context.
    """
    prompt = f"""
You are a question generator.

SYLLABUS CONTENT:
{context}

TASK:
Generate questions on {topic} using ONLY the syllabus content above.

[Additional requirements...]
"""
    return prompt
```

**What this does**:
- Takes retrieved context
- Embeds it in LLM prompt
- Instructs LLM to use only this context

### Step 4: Generate with LLM
```python
def generate_with_rag(topic: str, exam_type: str) -> list:
    """
    Complete RAG pipeline.
    """
    # 1. Retrieve
    context = retrieve_context(topic, exam_type)
    
    # 2. Augment
    prompt = create_prompt_with_context(topic, context)
    
    # 3. Generate
    response = gemini.generate(prompt)
    
    return parse_response(response)
```

**What this does**:
- Combines all steps
- Implements complete RAG flow
- Returns generated questions

## Testing RAG

### Test 1: Verify Retrieval
```python
context = retrieve_context("Mechanics", "JEE_MAIN")
assert len(context) > 0
assert "Newton" in context  # Should contain relevant content
```

### Test 2: Verify Generation Uses Context
```python
questions = generate_with_rag("Mechanics", "JEE_MAIN")
# Check if questions reference syllabus concepts
assert any("Newton" in q["question"] for q in questions)
```

### Test 3: Compare RAG vs Non-RAG
```python
# Without RAG
questions_no_rag = gemini.generate("Generate mechanics questions")

# With RAG
questions_with_rag = generate_with_rag("Mechanics", "JEE_MAIN")

# RAG questions should be more specific and accurate
```

## RAG Best Practices

1. **Chunk Size**: 200-500 words for optimal context
2. **Top-K**: Retrieve 3-5 chunks (balance context vs noise)
3. **Prompt Engineering**: Explicitly instruct LLM to use context
4. **Validation**: Verify generated content matches syllabus
5. **Caching**: Cache retrieved contexts for common topics

## Common RAG Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Irrelevant questions | Poor retrieval | Improve query, adjust top-k |
| Generic questions | LLM ignoring context | Strengthen prompt instructions |
| Slow generation | Too much context | Reduce chunk size or top-k |
| Hallucination | Insufficient context | Retrieve more relevant chunks |

## RAG Performance

- **Retrieval Time**: ~100ms
- **Generation Time**: ~2-3s
- **Total Time**: ~3s per request
- **Accuracy**: 90%+ syllabus alignment

## Advanced RAG Techniques

### 1. Hybrid Search
Combine vector search with keyword search for better retrieval.

### 2. Re-ranking
Re-order retrieved chunks by relevance before sending to LLM.

### 3. Iterative RAG
Generate â†’ Validate â†’ Retrieve more â†’ Regenerate if needed.

## Further Learning
- [RAG paper](https://arxiv.org/abs/2005.11401)
- [Google RAG documentation]
- [RAG best practices]
```

#### TROUBLESHOOTING.md
```markdown
# Troubleshooting Guide for [Task Name]

## Common Issues

### Issue 1: [Problem Description]

**Symptoms**:
- [What you see]
- [Error message]

**Possible Causes**:
1. [Cause 1]
2. [Cause 2]

**Solutions**:

**Solution 1**: [Fix for cause 1]
```bash
[Commands to fix]
```

**Solution 2**: [Fix for cause 2]
```bash
[Commands to fix]
```

**Verification**:
```bash
[Command to verify fix worked]
```

---

[Repeat for each common issue]

## Debug Checklist

When something doesn't work:
- [ ] Check all environment variables are set
- [ ] Verify services are running
- [ ] Check logs for errors
- [ ] Confirm dependencies are installed
- [ ] Test with sample data
- [ ] Review configuration steps

## Getting Help

1. **Check logs**:
   ```bash
   [Command to view logs]
   ```

2. **Enable debug mode**:
   ```bash
   [Command to enable debugging]
   ```

3. **Test individual components**:
   ```bash
   [Commands to test parts separately]
   ```

## Error Messages

### Error: [Error Message]
**Meaning**: [What this error means]
**Fix**: [How to resolve]

[Repeat for common errors]
```

## Components and Interfaces

### 1. Backend Tutorial Components (Tushar)

#### Day 1: Setup & Authentication
- **FastAPI Project Structure Prompt**: Generates complete project skeleton
- **Firebase Admin SDK Integration Prompt**: Sets up Firebase authentication
- **Parent Auth Endpoints Prompt**: Creates registration and login APIs
- **Testing with curl**: Standalone API testing without frontend

#### Day 2: User Onboarding
- **Parent Preferences API Prompt**: Language and settings endpoints
- **Child Profile API Prompt**: Child creation with one-child restriction
- **Exam Selection API Prompt**: Exam type and date management
- **Mock Request Testing**: JSON fixtures for API testing

#### Day 3: Diagnostic Test AI - Part 1
- **Vector Search Setup Prompt**: Vertex AI configuration
- **Syllabus Embedding Prompt**: Embedding generation code
- **Question Pattern Analysis Prompt**: 10-year pattern analysis
- **RAG Implementation Prompt**: Question generation with context

#### Day 4: Diagnostic Test - Part 2
- **Answer Submission Prompt**: Response handling endpoints
- **Scoring Engine Prompt**: Negative marking calculation
- **Gemini Flash Analytics Prompt**: AI-powered result analysis
- **API Documentation**: OpenAPI specs for frontend integration

#### Day 5: Study Planning
- **Schedule Generation Prompt**: Gemini Flash schedule creation
- **Adaptive Rescheduling Prompt**: Missed session handling
- **Practice Question Prompt**: Topic-specific question generation
- **Webhook Testing**: Simulating schedule triggers

#### Day 6: Parent Resources & Payment
- **Teaching Resources Prompt**: Multi-language content generation
- **Text-to-Speech Prompt**: Audio summary generation
- **Razorpay Integration Prompt**: Payment processing
- **Subscription Management Prompt**: Plan and renewal handling

#### Day 7: Testing & Deployment
- **i18n Setup Prompt**: Multi-language backend support
- **Analytics Dashboard Prompt**: Metrics and reporting
- **Dockerfile Prompt**: Container configuration
- **Cloud Run Deployment Prompt**: Production deployment

### 2. Frontend Tutorial Components (Vaishnavi)

#### Day 1: Setup & Authentication
- **Next.js Project Structure Prompt**: Complete frontend skeleton
- **Firebase Client SDK Prompt**: Client-side authentication
- **Registration UI Prompt**: Email, phone, Google Sign-In forms
- **Mock API Server**: Node.js server for standalone testing

#### Day 2: User Onboarding
- **Preferences Form Prompt**: Language and settings UI
- **Child Onboarding Flow Prompt**: Multi-step form
- **Exam Selection UI Prompt**: Exam picker with date calculation
- **Form Validation Prompt**: Client-side validation logic

#### Day 3: Diagnostic Test UI
- **Test Interface Prompt**: Question display and navigation
- **Timer Component Prompt**: Countdown timer
- **Answer Selection Prompt**: MCQ interaction
- **Progress Indicator Prompt**: Test completion tracking

#### Day 4: Analytics & Visualization
- **Analytics Dashboard Prompt**: Charts and graphs
- **Weak Topics Display Prompt**: Performance breakdown
- **Improvement Strategies Prompt**: Actionable recommendations
- **Mock Analytics Data**: JSON fixtures for testing

#### Day 5: Schedule & Practice
- **Schedule Display Prompt**: Calendar and daily tasks
- **Practice Module UI Prompt**: Question practice interface
- **Progress Tracking Prompt**: Mastery score visualization
- **Mock Schedule Data**: Sample schedules for testing

#### Day 6: Parent Dashboard & Payment
- **Parent Dashboard Prompt**: Overview and monitoring
- **Teaching Resources Viewer Prompt**: Notes and mindmaps
- **Audio Player Prompt**: Audio summary playback
- **Razorpay Checkout Prompt**: Payment flow UI

#### Day 7: Multi-language & Deployment
- **i18next Setup Prompt**: Frontend internationalization
- **Language Switcher Prompt**: Real-time language switching
- **Analytics Visualization Prompt**: Parent and student dashboards
- **Vercel/Firebase Hosting Prompt**: Frontend deployment

### 3. Mock Data and Testing Infrastructure

#### Backend Mock Data
```json
{
  "sample-requests": {
    "parent-registration": {
      "email": "parent@example.com",
      "language": "en",
      "preferences": {}
    },
    "child-profile": {
      "name": "Student Name",
      "age": 17,
      "grade": 12,
      "exam_type": "JEE_MAIN"
    }
  }
}
```

#### Frontend Mock API Server
```javascript
// mock-api-server.js
const express = require('express');
const app = express();

app.post('/api/auth/register/parent', (req, res) => {
  res.json({
    parent_id: "mock-parent-123",
    verification_required: false
  });
});

// ... more mock endpoints
```

#### Testing Scripts
```python
# test-scripts.py
import requests

def test_parent_registration():
    response = requests.post(
        'http://localhost:8000/api/auth/register/parent',
        json={'email': 'test@example.com', 'language': 'en'}
    )
    assert response.status_code == 200
    print("âœ“ Parent registration works")
```

## Data Models

### LLM Prompt Template

```markdown
ðŸ¤– PROMPT X.Y: [SHORT TITLE]

---
CONTEXT:
- Project: Mentor AI EdTech Platform
- Stack: [Specific tech for this prompt]
- File: [Where this code goes]

---
GENERATE:
[Exactly what to create]

---
REQUIREMENTS:
1. [Specific requirement]
2. [Specific requirement]
3. Include error handling
4. Add TypeScript types / Python type hints
5. Add detailed comments
6. [Additional requirements]

---
INTEGRATE WITH:
- [Other file/service this connects to]
- [API endpoint or database it uses]

---
OUTPUT FORMAT:
- Provide complete code (no placeholders)
- Include all imports
- Add example usage if applicable

---
TESTING:
[How to test this code standalone]
```

### Configuration Step Template

```markdown
## Configuration Steps

### Step 1: [Action Name]
**Command/Action:**
```bash
[Exact command]
```

**Why:** [Simple explanation]

**Verification:**
```bash
[How to verify it worked]
```

### Step 2: [Next Action]
...
```

### Testing Instruction Template

```markdown
## Testing Instructions

### Test 1: [Test Name]
**Do this:**
1. [Exact action]
2. [Exact action]

**You should see:**
```
[Expected output]
```

**If it fails:**
- Check: [Common issue 1]
- Check: [Common issue 2]
- Fix: [Solution]

### Test 2: [Next Test]
...
```

## Error Handling

### Common Issues and Solutions

#### Backend Issues
1. **Firebase Admin SDK initialization fails**
   - Cause: Missing service account key
   - Fix: Download key from Firebase Console â†’ Project Settings â†’ Service Accounts

2. **Vertex AI API not enabled**
   - Cause: API not activated in Google Cloud
   - Fix: Run `gcloud services enable aiplatform.googleapis.com`

3. **Gemini Flash rate limit exceeded**
   - Cause: Too many API calls
   - Fix: Implement exponential backoff retry logic

#### Frontend Issues
1. **Firebase client SDK authentication fails**
   - Cause: Incorrect Firebase config
   - Fix: Copy config from Firebase Console â†’ Project Settings â†’ Web App

2. **CORS errors when calling backend**
   - Cause: Backend CORS not configured
   - Fix: Add frontend URL to CORS allowed origins in FastAPI

3. **Mock API server not responding**
   - Cause: Server not started
   - Fix: Run `node mock-api-server.js` in terminal

### Error Response Format

All prompts should request this error format:

```python
# Backend
{
    "error": {
        "code": "AUTH_001",
        "message": "Invalid credentials",
        "details": {}
    },
    "timestamp": "2024-01-01T00:00:00Z"
}
```

```typescript
// Frontend
interface ErrorResponse {
  error: {
    code: string;
    message: string;
    details?: any;
  };
  timestamp: string;
}
```

## Testing Strategy

### Independent Testing (No Integration Required)

#### Backend Testing
1. **API Testing with curl**
   - Each prompt includes curl commands
   - Test endpoints without frontend
   - Verify JSON responses

2. **Python Test Scripts**
   - Automated API testing
   - Mock data fixtures
   - Response validation

3. **Firebase Emulator**
   - Local Firestore testing
   - No production data needed
   - Fast iteration

#### Frontend Testing
1. **Mock API Server**
   - Node.js Express server
   - Returns realistic mock data
   - Simulates backend responses

2. **Storybook Components**
   - Isolated component testing
   - Visual verification
   - No backend required

3. **Browser DevTools**
   - Console logging
   - Network tab inspection
   - Local storage verification

### Integration Testing (When Both Ready)

1. **End-to-End Flows**
   - Parent registration â†’ Child onboarding
   - Diagnostic test â†’ Analytics â†’ Schedule
   - Payment â†’ Subscription activation

2. **Cross-Browser Testing**
   - Chrome, Firefox, Safari
   - Mobile responsive testing

3. **Performance Testing**
   - API response times
   - Frontend load times
   - Database query optimization

## Deployment Strategy

### Backend Deployment (Tushar - Google Cloud Run)

1. **Local Development**
   - FastAPI with uvicorn
   - Firebase emulators
   - Environment variables in .env

2. **Staging Deployment to Cloud Run**
   - Create Dockerfile for FastAPI app
   - Build container with Cloud Build
   - Deploy to Cloud Run staging service
   - Configure staging environment variables
   - Test with staging Firebase project

3. **Production Deployment to Cloud Run**
   - Deploy to Cloud Run production service
   - Configure auto-scaling (min 1, max 100 instances)
   - Set up Cloud Load Balancer (optional)
   - Configure production environment variables
   - Set up custom domain with Cloud Run
   - Enable Cloud Logging and Monitoring
   - Configure secrets in Secret Manager

### Frontend Deployment (Vaishnavi - Firebase Hosting)

1. **Local Development**
   - Next.js dev server
   - Mock API server
   - Hot module replacement

2. **Staging Deployment to Firebase Hosting**
   - Build Next.js production bundle
   - Deploy to Firebase Hosting staging channel
   - Configure staging API endpoint (Cloud Run staging URL)
   - Test with preview URL

3. **Production Deployment to Firebase Hosting**
   - Deploy to Firebase Hosting production
   - Configure production API endpoint (Cloud Run production URL)
   - Set up custom domain in Firebase Console
   - Enable CDN caching
   - Configure SSL certificate (automatic with Firebase)
   - Set up Firebase Performance Monitoring

## Appendix Structure

### Backend Appendix

```markdown
# Backend Appendix

## Environment Variables
| Variable | Description | Where to Get |
|----------|-------------|--------------|
| FIREBASE_PROJECT_ID | Firebase project ID | Firebase Console |
| GOOGLE_CLOUD_PROJECT | GCP project ID | Cloud Console |
| ... | ... | ... |

## API Endpoints
| Endpoint | Method | Request | Response |
|----------|--------|---------|----------|
| /api/auth/register/parent | POST | {...} | {...} |
| ... | ... | ... | ... |

## Common Errors
| Error Code | Cause | Fix |
|------------|-------|-----|
| AUTH_001 | Invalid credentials | Check email/password |
| ... | ... | ... |

## Firebase Collections
- parents/
- students/
- diagnostic_tests/
- ...

## Testing Commands
```bash
# Run backend
uvicorn main:app --reload

# Test API
curl -X POST http://localhost:8000/api/auth/register/parent

# Run tests
pytest tests/
```
```

### Frontend Appendix

```markdown
# Frontend Appendix

## Environment Variables
| Variable | Description | Where to Get |
|----------|-------------|--------------|
| NEXT_PUBLIC_FIREBASE_API_KEY | Firebase API key | Firebase Console |
| NEXT_PUBLIC_API_URL | Backend API URL | Backend deployment |
| ... | ... | ... |

## API Integration
| Feature | Endpoint | Mock Data File |
|---------|----------|----------------|
| Registration | /api/auth/register/parent | mock-auth.json |
| ... | ... | ... |

## Common Issues
| Issue | Cause | Fix |
|-------|-------|-----|
| CORS error | Backend not configured | Add origin to CORS |
| ... | ... | ... |

## Component Structure
- components/
  - auth/
  - dashboard/
  - diagnostic/
  - ...

## Testing Commands
```bash
# Run frontend
npm run dev

# Run mock API
node mock-api-server.js

# Run tests
npm test
```
```

## Multi-Language Support

### Prompt Requirements for i18n

All content generation prompts should specify:

1. **Backend (Tushar)**
   - Accept `language` parameter in API requests
   - Pass language to Gemini Flash prompts
   - Return content in requested language
   - Support: English, Hindi, Marathi

2. **Frontend (Vaishnavi)**
   - Use i18next for UI translations
   - Language switcher component
   - Persist language preference
   - RTL support for future languages

### Translation Files Structure

```
frontend/
â””â”€â”€ public/
    â””â”€â”€ locales/
        â”œâ”€â”€ en/
        â”‚   â””â”€â”€ translation.json
        â”œâ”€â”€ hi/
        â”‚   â””â”€â”€ translation.json
        â””â”€â”€ mr/
            â””â”€â”€ translation.json
```

## Security Considerations

### Prompts Should Request

1. **Authentication**
   - JWT token validation
   - Firebase Auth integration
   - Role-based access control

2. **Data Validation**
   - Pydantic models (backend)
   - Zod schemas (frontend)
   - Input sanitization

3. **API Security**
   - CORS configuration
   - Rate limiting
   - Request size limits

4. **Payment Security**
   - Razorpay signature verification
   - No card data storage
   - PCI DSS compliance

5. **Environment Variables**
   - Never commit secrets
   - Use .env files
   - Separate dev/staging/prod

## Performance Optimization

### Prompts Should Request

1. **Backend**
   - Database query optimization
   - Caching (Redis or in-memory)
   - Async/await for I/O operations
   - Connection pooling

2. **Frontend**
   - Code splitting
   - Lazy loading
   - Image optimization
   - Memoization

3. **AI API Calls**
   - Retry logic with exponential backoff
   - Request batching
   - Response caching
   - Rate limit handling

## Daily Workflow

### For Tushar (Backend)

```
1. Open tushar-backend/DAY-X.md
2. Read overview and prerequisites
3. For each prompt:
   a. Copy prompt to LLM
   b. Copy generated code
   c. Create file at specified path
   d. Paste code and save
4. Follow configuration steps
5. Run testing commands
6. Verify with independent tests
7. (Optional) Check integration when Vaishnavi is ready
8. Move to next day
```

### For Vaishnavi (Frontend)

```
1. Open vaishnavi-frontend/DAY-X.md
2. Read overview and prerequisites
3. Start mock API server (if needed)
4. For each prompt:
   a. Copy prompt to LLM
   b. Copy generated code
   c. Create file at specified path
   d. Paste code and save
5. Follow configuration steps
6. Test in browser with mock data
7. Verify with independent tests
8. (Optional) Check integration when Tushar is ready
9. Move to next day
```

## Success Criteria

### Per Day
- All prompts executed successfully
- All configuration steps completed
- All independent tests passing
- Code committed to version control

### Per Week
- All 7 days completed
- Backend API fully functional
- Frontend UI fully functional
- (Optional) Full integration tested
- Platform deployed to staging/production

## Professional UI/UX Design Guidelines

### Design Philosophy for Mentor AI

The platform should feel:
- **Professional**: Like a premium EdTech product, not a hobby project
- **Trustworthy**: Parents are investing in their child's future
- **Modern**: Contemporary design that appeals to 2024+ users
- **Accessible**: Easy to use for parents and students of all tech levels
- **Inspiring**: Motivating and encouraging, not overwhelming

### Visual Design System

#### Color Palette

**Primary Colors** (Education & Trust):
```css
--primary-blue: #2563EB;      /* Main actions, links */
--primary-dark: #1E40AF;      /* Hover states */
--primary-light: #DBEAFE;     /* Backgrounds */
```

**Secondary Colors** (Success & Growth):
```css
--success-green: #10B981;     /* Achievements, progress */
--warning-amber: #F59E0B;     /* Warnings, attention */
--error-red: #EF4444;         /* Errors, weak areas */
```

**Neutral Colors** (Professional):
```css
--gray-900: #111827;          /* Headings */
--gray-700: #374151;          /* Body text */
--gray-500: #6B7280;          /* Secondary text */
--gray-200: #E5E7EB;          /* Borders */
--gray-50: #F9FAFB;           /* Backgrounds */
--white: #FFFFFF;             /* Cards, surfaces */
```

**Accent Colors** (Subjects):
```css
--physics-purple: #8B5CF6;    /* Physics topics */
--chemistry-teal: #14B8A6;    /* Chemistry topics */
--math-orange: #F97316;       /* Math topics */
--biology-green: #22C55E;     /* Biology topics */
```

#### Typography

**Font Stack**:
```css
--font-primary: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
--font-display: 'Poppins', 'Inter', sans-serif;
--font-mono: 'JetBrains Mono', 'Fira Code', monospace;
```

**Type Scale**:
```css
--text-xs: 0.75rem;    /* 12px - Labels */
--text-sm: 0.875rem;   /* 14px - Secondary text */
--text-base: 1rem;     /* 16px - Body text */
--text-lg: 1.125rem;   /* 18px - Large body */
--text-xl: 1.25rem;    /* 20px - Small headings */
--text-2xl: 1.5rem;    /* 24px - Headings */
--text-3xl: 1.875rem;  /* 30px - Large headings */
--text-4xl: 2.25rem;   /* 36px - Hero text */
```

#### Spacing System

```css
--space-1: 0.25rem;   /* 4px */
--space-2: 0.5rem;    /* 8px */
--space-3: 0.75rem;   /* 12px */
--space-4: 1rem;      /* 16px */
--space-6: 1.5rem;    /* 24px */
--space-8: 2rem;      /* 32px */
--space-12: 3rem;     /* 48px */
--space-16: 4rem;     /* 64px */
```

#### Border Radius

```css
--radius-sm: 0.375rem;   /* 6px - Buttons, inputs */
--radius-md: 0.5rem;     /* 8px - Cards */
--radius-lg: 0.75rem;    /* 12px - Modals */
--radius-xl: 1rem;       /* 16px - Hero sections */
--radius-full: 9999px;   /* Pills, avatars */
```

#### Shadows

```css
--shadow-sm: 0 1px 2px 0 rgb(0 0 0 / 0.05);
--shadow-md: 0 4px 6px -1px rgb(0 0 0 / 0.1);
--shadow-lg: 0 10px 15px -3px rgb(0 0 0 / 0.1);
--shadow-xl: 0 20px 25px -5px rgb(0 0 0 / 0.1);
```

### Component Design Patterns

#### 1. Dashboard Cards

**Design Requirements**:
- Clean white background with subtle shadow
- Clear hierarchy with bold headings
- Visual indicators (icons, colors) for quick scanning
- Hover effects for interactivity
- Responsive padding

**Example Prompt Addition**:
```
DESIGN REQUIREMENTS:
- Use white background with shadow-md
- Add subtle hover effect (shadow-lg on hover)
- Include icon with primary-blue color
- Use text-2xl for heading, text-base for content
- Add border-l-4 with accent color for category
- Responsive: full width on mobile, grid on desktop
```

#### 2. Progress Visualizations

**Design Requirements**:
- Use color-coded progress bars (green for good, amber for moderate, red for weak)
- Smooth animations for progress changes
- Clear percentage labels
- Visual comparison (before/after)

**Example Prompt Addition**:
```
DESIGN REQUIREMENTS:
- Animated progress bars with transition-all duration-500
- Color coding: green (>80%), amber (60-80%), red (<60%)
- Show percentage label on right
- Add small trend indicator (â†‘ improved, â†“ declined)
- Use gradient fills for visual appeal
```

#### 3. Forms and Inputs

**Design Requirements**:
- Large, easy-to-tap inputs (min 44px height)
- Clear labels above inputs
- Helpful placeholder text
- Inline validation with icons
- Disabled state clearly visible

**Example Prompt Addition**:
```
DESIGN REQUIREMENTS:
- Input height: 44px minimum
- Border: 2px solid gray-200, focus: primary-blue
- Label: text-sm font-medium text-gray-700
- Error state: border-error-red with error icon
- Success state: border-success-green with checkmark
- Smooth focus ring with ring-2 ring-primary-blue
```

#### 4. Buttons

**Design Requirements**:
- Primary: Bold, high contrast
- Secondary: Outlined, subtle
- Disabled: Grayed out, not clickable
- Loading: Spinner animation
- Consistent sizing

**Example Prompt Addition**:
```
DESIGN REQUIREMENTS:
- Primary button: bg-primary-blue text-white hover:bg-primary-dark
- Secondary button: border-2 border-primary-blue text-primary-blue hover:bg-primary-light
- Disabled: opacity-50 cursor-not-allowed
- Loading: Show spinner icon with animate-spin
- Padding: px-6 py-3 for comfortable click target
- Border radius: radius-md
- Font: font-medium text-base
```

#### 5. Navigation

**Design Requirements**:
- Sticky header for easy access
- Clear active state
- Mobile-friendly hamburger menu
- Smooth transitions

**Example Prompt Addition**:
```
DESIGN REQUIREMENTS:
- Sticky top navigation with backdrop-blur
- Active link: border-b-2 border-primary-blue
- Mobile: Hamburger menu with slide-in drawer
- Logo: 40px height, left-aligned
- User avatar: 32px circle, right-aligned
- Smooth transitions: transition-all duration-300
```

### Page-Specific Design Guidelines

#### Parent Dashboard

**Hero Section**:
```
- Large welcome message with parent name
- Quick stats cards (study hours, progress, upcoming tests)
- Call-to-action button (View detailed report)
- Background: Subtle gradient from primary-light to white
```

**Progress Section**:
```
- Subject-wise progress cards with color coding
- Interactive charts (bar, line, radar)
- Weak areas highlighted in red with action buttons
- Strong areas celebrated with green badges
```

**Schedule Section**:
```
- Calendar view with color-coded tasks
- Today's tasks prominently displayed
- Completion checkboxes with smooth animations
- Upcoming milestones timeline
```

#### Student Dashboard

**Motivational Header**:
```
- Personalized greeting with time of day
- Streak counter with fire emoji ðŸ”¥
- Daily goal progress bar
- Encouraging message based on progress
```

**Test Section**:
```
- Large, prominent "Start Diagnostic Test" button
- Test details card (duration, questions, format)
- Previous test scores with trend indicators
- Practice recommendations based on weak areas
```

**Practice Section**:
```
- Topic cards with mastery percentage
- Visual difficulty indicators (easy/medium/hard)
- "Continue practicing" for in-progress topics
- Gamification elements (badges, points)
```

#### Diagnostic Test Interface

**Question Display**:
```
- Clean, distraction-free layout
- Large, readable question text (text-lg)
- Clear option buttons with hover effects
- Visual feedback on selection (border highlight)
- Smooth transitions between questions
```

**Navigation**:
```
- Question palette (grid of question numbers)
- Color coding: answered (green), unanswered (gray), marked (amber)
- Timer with warning when < 5 minutes
- Submit button with confirmation modal
```

#### Analytics Dashboard

**Performance Overview**:
```
- Large score display with circular progress
- Subject breakdown with radar chart
- Topic-wise heatmap (red to green)
- Comparison with average scores
```

**Insights Section**:
```
- AI-generated insights in card format
- Icon for each insight type (ðŸ’¡ tip, âš ï¸ warning, âœ… strength)
- Actionable recommendations with buttons
- Parent guidance in separate, highlighted section
```

### Animation and Micro-interactions

#### Loading States

```javascript
// Skeleton loaders for content
<div className="animate-pulse">
  <div className="h-4 bg-gray-200 rounded w-3/4"></div>
  <div className="h-4 bg-gray-200 rounded w-1/2 mt-2"></div>
</div>

// Spinner for actions
<svg className="animate-spin h-5 w-5" viewBox="0 0 24 24">
  <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
  <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
</svg>
```

#### Success Animations

```javascript
// Checkmark animation on success
<motion.div
  initial={{ scale: 0 }}
  animate={{ scale: 1 }}
  transition={{ type: "spring", stiffness: 260, damping: 20 }}
>
  <CheckCircleIcon className="h-16 w-16 text-success-green" />
</motion.div>
```

#### Page Transitions

```javascript
// Smooth page transitions
<motion.div
  initial={{ opacity: 0, y: 20 }}
  animate={{ opacity: 1, y: 0 }}
  exit={{ opacity: 0, y: -20 }}
  transition={{ duration: 0.3 }}
>
  {children}
</motion.div>
```

### Responsive Design

#### Breakpoints

```css
--screen-sm: 640px;   /* Mobile landscape */
--screen-md: 768px;   /* Tablet */
--screen-lg: 1024px;  /* Desktop */
--screen-xl: 1280px;  /* Large desktop */
```

#### Mobile-First Approach

```javascript
// Stack on mobile, grid on desktop
<div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6">
  {cards.map(card => <Card key={card.id} {...card} />)}
</div>

// Hide on mobile, show on desktop
<div className="hidden lg:block">
  <Sidebar />
</div>

// Full width on mobile, constrained on desktop
<div className="w-full lg:max-w-4xl mx-auto">
  <Content />
</div>
```

### Accessibility

#### ARIA Labels

```javascript
<button
  aria-label="Start diagnostic test"
  aria-describedby="test-description"
>
  Start Test
</button>
```

#### Keyboard Navigation

```javascript
// Focus visible states
<button className="focus:outline-none focus:ring-2 focus:ring-primary-blue focus:ring-offset-2">
  Click me
</button>

// Skip to main content
<a href="#main-content" className="sr-only focus:not-sr-only">
  Skip to main content
</a>
```

#### Color Contrast

- Ensure WCAG AA compliance (4.5:1 for normal text)
- Use tools like WebAIM Contrast Checker
- Don't rely on color alone for information

### Enhanced LLM Prompts for UI Components

#### Example: Enhanced Registration Component Prompt

```
Create a professional, modern React registration component for parent onboarding.

CONTEXT:
- File: frontend/components/auth/ParentRegistration.tsx
- Tech: Next.js 14, TypeScript, Tailwind CSS, Framer Motion
- Backend API: http://localhost:8000/api/auth/register/parent

DESIGN REQUIREMENTS:
1. Layout:
   - Centered card with max-w-md
   - White background with shadow-xl
   - Rounded corners (radius-lg)
   - Padding: p-8
   - Responsive: full screen on mobile, card on desktop

2. Typography:
   - Heading: text-3xl font-bold text-gray-900
   - Subheading: text-base text-gray-600
   - Labels: text-sm font-medium text-gray-700
   - Error text: text-sm text-error-red

3. Form Inputs:
   - Height: 44px minimum
   - Border: 2px solid gray-200
   - Focus: border-primary-blue with ring-2
   - Error state: border-error-red
   - Placeholder: text-gray-400
   - Smooth transitions: transition-all duration-200

4. Buttons:
   - Primary: bg-primary-blue hover:bg-primary-dark
   - Google: bg-white border-2 border-gray-300 with Google logo
   - Full width on mobile, auto on desktop
   - Loading state: Show spinner, disable button
   - Padding: px-6 py-3

5. Animations:
   - Page entrance: fade in from bottom
   - Form validation: shake on error
   - Success: checkmark animation
   - Loading: spinner animation

6. Accessibility:
   - ARIA labels on all inputs
   - Keyboard navigation support
   - Focus visible states
   - Error announcements for screen readers

7. Visual Enhancements:
   - Icons for email, phone, Google (use Heroicons)
   - Success/error toast notifications
   - Password strength indicator
   - Language selector with flag icons

GENERATE:
[Rest of prompt...]

ADDITIONAL REQUIREMENTS:
- Use Framer Motion for animations
- Include loading skeletons
- Add micro-interactions (hover effects, focus states)
- Implement form validation with visual feedback
- Show success state with confetti animation (optional)
```

### UI Component Library Recommendations

For consistent, professional UI:

1. **Headless UI** (by Tailwind Labs)
   - Accessible components
   - Unstyled, fully customizable
   - Perfect for custom designs

2. **Heroicons**
   - Beautiful, consistent icons
   - Matches Tailwind design system
   - Free and open source

3. **Framer Motion**
   - Smooth animations
   - Easy to use
   - Production-ready

4. **Recharts**
   - Beautiful charts
   - Responsive
   - Customizable

### Professional Polish Checklist

For each component, ensure:
- [ ] Consistent spacing using design system
- [ ] Proper color contrast (WCAG AA)
- [ ] Smooth animations and transitions
- [ ] Loading states for async operations
- [ ] Error states with helpful messages
- [ ] Empty states with illustrations
- [ ] Hover effects on interactive elements
- [ ] Focus states for keyboard navigation
- [ ] Responsive design (mobile, tablet, desktop)
- [ ] Consistent typography scale
- [ ] Proper use of shadows and depth
- [ ] Icons for visual hierarchy
- [ ] Success feedback for user actions

## Google Cloud Run Deployment Guide

### What is Cloud Run?

Cloud Run is Google's serverless container platform that:
- Automatically scales from 0 to many instances
- Only charges for actual usage (pay-per-request)
- Handles HTTPS, load balancing, and SSL automatically
- Supports any language/framework via Docker containers

### Why Cloud Run for Mentor AI?

1. **Cost-Effective**: Pay only when API is used, scales to zero
2. **Auto-Scaling**: Handles traffic spikes automatically
3. **Fully Managed**: No server management needed
4. **Fast Deployment**: Deploy in minutes with Cloud Build
5. **Integrated**: Works seamlessly with Firebase, Vertex AI, Gemini

### Cloud Run Architecture for Mentor AI

```
User Request (Frontend)
      â†“
Firebase Hosting (CDN)
      â†“
Cloud Run Service (Backend API)
      â†“
â”œâ”€ Firebase Firestore (Database)
â”œâ”€ Vertex AI (Vector Search)
â”œâ”€ Gemini Flash (AI Generation)
â””â”€ Razorpay (Payments)
```

### Deployment Steps (Day 10)

#### Step 1: Create Dockerfile

**PROMPT: Generate Cloud Run Dockerfile**
```
Create a production-ready Dockerfile for FastAPI deployment to Google Cloud Run.

CONTEXT:
- Project: Mentor AI Backend
- Framework: FastAPI with Python 3.11
- Platform: Google Cloud Run
- Port: 8080 (Cloud Run requirement)

GENERATE:

Dockerfile with:

1. Multi-stage build for smaller image size
2. Python 3.11 slim base image
3. Install dependencies from requirements.txt
4. Copy application code
5. Expose port 8080
6. Run with uvicorn
7. Health check endpoint
8. Non-root user for security

REQUIREMENTS:
- Optimize for fast cold starts
- Minimize image size
- Include security best practices
- Add health check
- Configure for Cloud Run

OUTPUT:
Complete Dockerfile ready for Cloud Run deployment.
```

**Expected Dockerfile**:
```dockerfile
# Multi-stage build
FROM python:3.11-slim as builder

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# Final stage
FROM python:3.11-slim

WORKDIR /app

# Copy dependencies from builder
COPY --from=builder /root/.local /root/.local

# Copy application code
COPY . .

# Make sure scripts are executable
ENV PATH=/root/.local/bin:$PATH

# Create non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Expose Cloud Run port
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8080/health')"

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]
```

#### Step 2: Create Cloud Build Configuration

**PROMPT: Generate cloudbuild.yaml**
```
Create Cloud Build configuration for automated deployment to Cloud Run.

CONTEXT:
- Project: Mentor AI Backend
- Service: mentor-ai-backend
- Region: us-central1

GENERATE:

cloudbuild.yaml with:

1. Build Docker image
2. Push to Google Container Registry
3. Deploy to Cloud Run
4. Run tests before deployment
5. Configure environment variables
6. Set up service account

REQUIREMENTS:
- Automated CI/CD pipeline
- Run tests before deploy
- Deploy only if tests pass
- Configure Cloud Run settings

OUTPUT:
Complete cloudbuild.yaml file.
```

**Expected cloudbuild.yaml**:
```yaml
steps:
  # Build the container image
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/$PROJECT_ID/mentor-ai-backend:$COMMIT_SHA', '.']
  
  # Push the container image to Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/$PROJECT_ID/mentor-ai-backend:$COMMIT_SHA']
  
  # Run tests
  - name: 'gcr.io/$PROJECT_ID/mentor-ai-backend:$COMMIT_SHA'
    entrypoint: 'pytest'
    args: ['tests/']
  
  # Deploy to Cloud Run
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      - 'run'
      - 'deploy'
      - 'mentor-ai-backend'
      - '--image=gcr.io/$PROJECT_ID/mentor-ai-backend:$COMMIT_SHA'
      - '--region=us-central1'
      - '--platform=managed'
      - '--allow-unauthenticated'
      - '--set-env-vars=ENVIRONMENT=production'
      - '--min-instances=1'
      - '--max-instances=100'
      - '--memory=2Gi'
      - '--cpu=2'
      - '--timeout=300'

images:
  - 'gcr.io/$PROJECT_ID/mentor-ai-backend:$COMMIT_SHA'

options:
  machineType: 'N1_HIGHCPU_8'
```

#### Step 3: Configure Environment Variables

Use Google Secret Manager for sensitive data:

```bash
# Create secrets
gcloud secrets create firebase-credentials --data-file=service-account.json
gcloud secrets create razorpay-key-secret --data-file=-

# Grant Cloud Run access to secrets
gcloud secrets add-iam-policy-binding firebase-credentials \
  --member=serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com \
  --role=roles/secretmanager.secretAccessor
```

Update Cloud Run service to use secrets:
```bash
gcloud run services update mentor-ai-backend \
  --update-secrets=FIREBASE_CREDENTIALS=firebase-credentials:latest \
  --update-secrets=RAZORPAY_KEY_SECRET=razorpay-key-secret:latest
```

#### Step 4: Deploy to Cloud Run

```bash
# Enable required APIs
gcloud services enable run.googleapis.com
gcloud services enable cloudbuild.googleapis.com
gcloud services enable containerregistry.googleapis.com

# Deploy using Cloud Build
gcloud builds submit --config cloudbuild.yaml

# Or deploy directly
gcloud run deploy mentor-ai-backend \
  --source . \
  --region us-central1 \
  --allow-unauthenticated
```

#### Step 5: Configure Custom Domain

```bash
# Map custom domain
gcloud run domain-mappings create \
  --service mentor-ai-backend \
  --domain api.mentorai.com \
  --region us-central1

# Get DNS records to configure
gcloud run domain-mappings describe \
  --domain api.mentorai.com \
  --region us-central1
```

### Cloud Run Configuration

**Recommended Settings**:
- **Min Instances**: 1 (avoid cold starts)
- **Max Instances**: 100 (handle traffic spikes)
- **Memory**: 2 GiB (for AI operations)
- **CPU**: 2 vCPU (for concurrent requests)
- **Timeout**: 300s (for long AI operations)
- **Concurrency**: 80 (requests per instance)

### Monitoring and Logging

```bash
# View logs
gcloud run services logs read mentor-ai-backend --region us-central1

# View metrics in Cloud Console
# Navigate to: Cloud Run â†’ mentor-ai-backend â†’ Metrics
```

### Cost Optimization

1. **Use Min Instances Wisely**: Set to 1 for production, 0 for dev
2. **Right-Size Resources**: Start with 1 CPU, scale up if needed
3. **Cache Responses**: Reduce AI API calls
4. **Optimize Cold Starts**: Keep Docker image small

**Estimated Costs** (for 10,000 requests/day):
- Cloud Run: ~$10/month
- Container Registry: ~$5/month
- Total: ~$15/month

### Testing Deployed API

```bash
# Get Cloud Run URL
gcloud run services describe mentor-ai-backend \
  --region us-central1 \
  --format 'value(status.url)'

# Test health endpoint
curl https://mentor-ai-backend-xxx-uc.a.run.app/health

# Test API endpoint
curl -X POST https://mentor-ai-backend-xxx-uc.a.run.app/api/auth/register/parent \
  -H "Content-Type: application/json" \
  -d '{"email": "test@example.com", "language": "en"}'
```

### Troubleshooting Cloud Run

| Issue | Cause | Solution |
|-------|-------|----------|
| Container fails to start | Port not 8080 | Update Dockerfile to expose 8080 |
| 503 Service Unavailable | Cold start timeout | Increase timeout or use min instances |
| Out of memory | Insufficient memory | Increase memory allocation |
| Permission denied | Missing IAM roles | Grant required roles to service account |
| Secrets not accessible | IAM policy missing | Add secretAccessor role |

## API Contract Specification

### Purpose
This section defines the exact API contract between backend and frontend to ensure perfect alignment and prevent integration errors.

### API Design Principles

1. **RESTful**: Follow REST conventions
2. **Versioned**: All endpoints start with `/api/v1/`
3. **Consistent**: Same patterns across all endpoints
4. **Typed**: Clear request/response types
5. **Documented**: OpenAPI/Swagger documentation

### Complete API Endpoint List

#### Authentication Endpoints

**POST /api/v1/auth/register/parent**
```typescript
// Request
interface ParentRegistrationRequest {
  email?: string;
  phone?: string;
  google_token?: string;
  language: "en" | "hi" | "mr";
}

// Response (200 OK)
interface ParentRegistrationResponse {
  parent_id: string;
  verification_required: boolean;
  message: string;
}

// Error (400 Bad Request)
interface ErrorResponse {
  error: {
    code: string;
    message: string;
    details?: any;
  };
}
```

**POST /api/v1/auth/login**
```typescript
// Request
interface LoginRequest {
  firebase_token: string;
}

// Response (200 OK)
interface LoginResponse {
  user_type: "parent" | "student";
  profile: ParentProfile | StudentProfile;
  token: string;
}
```

**POST /api/v1/auth/verify**
```typescript
// Request
interface VerifyRequest {
  parent_id: string;
  code: string;
}

// Response (200 OK)
interface VerifyResponse {
  verified: boolean;
  token: string;
}
```

#### User Management Endpoints

**POST /api/v1/users/parent/preferences**
```typescript
// Request
interface PreferencesRequest {
  language: string;
  notification_preferences: {
    email: boolean;
    sms: boolean;
    push: boolean;
  };
  study_times: string[];  // ["morning", "evening"]
  educational_background: string;
}

// Response (200 OK)
interface PreferencesResponse {
  parent_id: string;
  preferences: PreferencesRequest;
  updated_at: string;  // ISO 8601
}
```

**POST /api/v1/users/child**
```typescript
// Request
interface ChildProfileRequest {
  name: string;
  age: number;
  grade: number;
  school: string;
  prep_status: "NOT_STARTED" | "SELF_STUDYING" | "ATTENDING_COACHING";
}

// Response (200 OK)
interface ChildProfileResponse {
  child_id: string;
  student_credentials: {
    username: string;
    password: string;
  };
  profile: ChildProfileRequest;
}
```

**PUT /api/v1/users/child/{child_id}/exam**
```typescript
// Request
interface ExamSelectionRequest {
  exam_type: "JEE_MAIN" | "JEE_ADVANCED" | "NEET" | "MHT_CET";
  exam_session: string;  // "January 2024"
  exam_year: number;
  exam_date: string;  // ISO 8601
}

// Response (200 OK)
interface ExamSelectionResponse {
  child_id: string;
  exam_details: ExamSelectionRequest;
  days_remaining: number;
}
```

#### Diagnostic Test Endpoints

**POST /api/v1/diagnostic/generate**
```typescript
// Request
interface DiagnosticGenerateRequest {
  child_id: string;
  exam_type: string;
}

// Response (200 OK)
interface DiagnosticGenerateResponse {
  test_id: string;
  questions: Question[];
  duration_minutes: number;
  instructions: string;
}

interface Question {
  id: string;
  topic: string;
  subtopic: string;
  difficulty: "easy" | "medium" | "hard";
  type: "MCQ" | "MULTIPLE_CORRECT" | "INTEGER";
  question: string;
  options: {
    A: string;
    B: string;
    C: string;
    D: string;
  };
  weightage: number;
}
```

**POST /api/v1/diagnostic/submit-answer**
```typescript
// Request
interface SubmitAnswerRequest {
  test_id: string;
  question_id: string;
  answer: string | string[];
}

// Response (200 OK)
interface SubmitAnswerResponse {
  saved: boolean;
  question_id: string;
}
```

**POST /api/v1/diagnostic/submit-test**
```typescript
// Request
interface SubmitTestRequest {
  test_id: string;
}

// Response (200 OK)
interface SubmitTestResponse {
  test_id: string;
  overall_score: number;
  topic_scores: TopicScore[];
  analytics_id: string;
}

interface TopicScore {
  topic: string;
  score: number;
  total_questions: number;
  correct_answers: number;
  category: "WEAK" | "MODERATE" | "STRONG";
}
```

#### Analytics Endpoints

**GET /api/v1/analytics/{analytics_id}**
```typescript
// Response (200 OK)
interface AnalyticsResponse {
  analytics_id: string;
  test_id: string;
  child_id: string;
  weak_subjects: SubjectAnalysis[];
  weak_topics: TopicAnalysis[];
  strategies: ImprovementStrategy[];
  visual_charts: ChartData[];
  generated_at: string;  // ISO 8601
}

interface SubjectAnalysis {
  subject: string;
  score: number;
  weightage: number;
  weak_topics: string[];
  reason: string;
}

interface TopicAnalysis {
  topic: string;
  score: number;
  weightage: number;
  priority: "HIGH" | "MEDIUM" | "LOW";
  reason: string;
}

interface ImprovementStrategy {
  topic: string;
  weightage: number;
  focus_areas: string[];
  study_techniques: string[];
  estimated_time: string;
  parent_guidance: string;
}
```

#### Schedule Endpoints

**POST /api/v1/schedule/generate**
```typescript
// Request
interface ScheduleGenerateRequest {
  child_id: string;
  analytics_id: string;
  exam_date: string;  // ISO 8601
  preferences: {
    study_hours_per_day: number;
    preferred_times: string[];
    current_prep_status: string;
  };
}

// Response (200 OK)
interface ScheduleGenerateResponse {
  schedule_id: string;
  daily_tasks: DailyTask[];
  milestones: Milestone[];
  generated_at: string;  // ISO 8601
}

interface DailyTask {
  day: number;
  date: string;  // ISO 8601
  topics: string[];
  activities: Activity[];
  total_hours: number;
  milestone?: string;
}

interface Activity {
  type: "CONCEPT_LEARNING" | "PRACTICE" | "REVIEW" | "MOCK_TEST";
  topic: string;
  duration_hours: number;
  weightage?: number;
  description: string;
}

interface Milestone {
  week: number;
  goal: string;
  topics_covered: string[];
  assessment: string;
}
```

**GET /api/v1/schedule/{schedule_id}**
```typescript
// Response (200 OK)
interface ScheduleResponse {
  schedule_id: string;
  child_id: string;
  schedule: ScheduleGenerateResponse;
  progress: {
    completed_tasks: number;
    total_tasks: number;
    completion_percentage: number;
  };
  adjustments: ScheduleAdjustment[];
}

interface ScheduleAdjustment {
  date: string;  // ISO 8601
  reason: "MISSED_SESSION" | "PRACTICE_TEST_PERFORMANCE";
  changes: string[];
}
```

**POST /api/v1/schedule/reschedule**
```typescript
// Request
interface RescheduleRequest {
  schedule_id: string;
  reason: "MISSED_SESSION" | "PRACTICE_TEST_PERFORMANCE";
  trigger_data: {
    missed_sessions?: number;
    new_weak_topics?: string[];
    improved_topics?: string[];
  };
}

// Response (200 OK)
interface RescheduleResponse {
  schedule_id: string;
  updated_schedule: ScheduleGenerateResponse;
  changes: string[];
  strategy: "MINOR_ADJUSTMENT" | "MAJOR_REPLAN" | "PRIORITY_BOOST";
}
```

#### Question Generation Endpoints

**POST /api/v1/questions/generate**
```typescript
// Request
interface QuestionGenerateRequest {
  topic: string;
  exam_type: string;
  difficulty: "easy" | "medium" | "hard";
  count: number;
}

// Response (200 OK)
interface QuestionGenerateResponse {
  questions: Question[];
  generated_at: string;  // ISO 8601
}
```

**POST /api/v1/questions/practice-session**
```typescript
// Request
interface PracticeSessionRequest {
  child_id: string;
  topic: string;
  session_type: "PRACTICE" | "REVIEW";
}

// Response (200 OK)
interface PracticeSessionResponse {
  session_id: string;
  questions: Question[];
  duration_minutes: number;
}
```

#### Teaching Resources Endpoints

**GET /api/v1/teaching/notes/{topic}**
```typescript
// Query Parameters
interface TeachingNotesQuery {
  language: string;
  exam_type: string;
}

// Response (200 OK)
interface TeachingNotesResponse {
  topic: string;
  language: string;
  notes: string;  // Markdown format
  examples: string[];
  formulas: string[];
  generated_at: string;  // ISO 8601
}
```

**GET /api/v1/teaching/methodology/{topic}**
```typescript
// Response (200 OK)
interface TeachingMethodologyResponse {
  topic: string;
  teaching_guide: string;
  common_difficulties: string[];
  activities: string[];
  teaching_sequence: string[];
}
```

**GET /api/v1/teaching/mindmap/{topic}**
```typescript
// Query Parameters
interface MindmapQuery {
  language: string;
}

// Response (200 OK)
interface MindmapResponse {
  topic: string;
  language: string;
  mindmap_data: MindmapNode;
  image_url?: string;
}

interface MindmapNode {
  id: string;
  label: string;
  children: MindmapNode[];
}
```

**GET /api/v1/teaching/audio-summary/{topic}**
```typescript
// Query Parameters
interface AudioSummaryQuery {
  language: string;
}

// Response (200 OK)
interface AudioSummaryResponse {
  topic: string;
  language: string;
  audio_url: string;
  transcript: string;
  duration_seconds: number;
}
```

#### Payment Endpoints

**POST /api/v1/payment/create-order**
```typescript
// Request
interface PaymentOrderRequest {
  parent_id: string;
  plan_id: string;
}

// Response (200 OK)
interface PaymentOrderResponse {
  order_id: string;
  razorpay_order_id: string;
  amount: number;
  currency: string;
  razorpay_key_id: string;  // For frontend integration
}
```

**POST /api/v1/payment/verify**
```typescript
// Request
interface PaymentVerifyRequest {
  order_id: string;
  razorpay_payment_id: string;
  razorpay_signature: string;
}

// Response (200 OK)
interface PaymentVerifyResponse {
  verified: boolean;
  subscription_active: boolean;
  subscription_expiry: string;  // ISO 8601
}
```

**GET /api/v1/payment/history/{parent_id}**
```typescript
// Response (200 OK)
interface PaymentHistoryResponse {
  transactions: Transaction[];
  current_plan: string;
  renewal_date: string;  // ISO 8601
}

interface Transaction {
  transaction_id: string;
  amount: number;
  status: "SUCCESS" | "FAILED" | "PENDING";
  date: string;  // ISO 8601
  plan: string;
}
```

#### Progress Tracking Endpoints

**GET /api/v1/progress/{student_id}**
```typescript
// Response (200 OK)
interface ProgressResponse {
  student_id: string;
  overall_progress: {
    completion_percentage: number;
    mastery_score: number;
    topics_completed: number;
    total_topics: number;
  };
  subject_progress: SubjectProgress[];
  topic_progress: TopicProgress[];
  timeline: ProgressTimeline[];
}

interface SubjectProgress {
  subject: string;
  completion_percentage: number;
  mastery_score: number;
}

interface TopicProgress {
  topic: string;
  completion_percentage: number;
  mastery_score: number;
  status: "NOT_STARTED" | "IN_PROGRESS" | "COMPLETED" | "MASTERED";
  time_spent_hours: number;
  practice_sessions: number;
  last_practiced: string;  // ISO 8601
}

interface ProgressTimeline {
  date: string;  // ISO 8601
  topics_covered: string[];
  time_spent_hours: number;
  questions_attempted: number;
  questions_correct: number;
}
```

#### Dashboard Endpoints

**GET /api/v1/dashboard/parent/{parent_id}**
```typescript
// Response (200 OK)
interface ParentDashboardResponse {
  parent_id: string;
  child_profile: ChildProfile;
  progress_summary: {
    overall_completion: number;
    weak_areas: string[];
    strong_areas: string[];
    study_streak_days: number;
    daily_study_hours: number;
  };
  schedule_summary: {
    today_tasks: DailyTask;
    upcoming_milestones: Milestone[];
  };
  analytics_summary: {
    recent_test_score: number;
    improvement_trend: "IMPROVING" | "STABLE" | "DECLINING";
  };
}
```

**GET /api/v1/dashboard/student/{student_id}**
```typescript
// Response (200 OK)
interface StudentDashboardResponse {
  student_id: string;
  exam_details: {
    exam_type: string;
    exam_date: string;  // ISO 8601
    days_remaining: number;
  };
  today_tasks: DailyTask;
  progress_summary: {
    topics_mastered: number;
    current_streak: number;
    total_study_hours: number;
  };
  diagnostic_status: {
    completed: boolean;
    score?: number;
    analytics_available: boolean;
  };
  practice_recommendations: {
    topic: string;
    reason: string;
  }[];
}
```

### Frontend API Client

**TypeScript API Client Template**:

```typescript
// lib/api-client.ts

const API_BASE_URL = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000';

class APIClient {
  private baseURL: string;
  private token: string | null = null;

  constructor(baseURL: string) {
    this.baseURL = baseURL;
  }

  setToken(token: string) {
    this.token = token;
  }

  private async request<T>(
    endpoint: string,
    options: RequestInit = {}
  ): Promise<T> {
    const url = `${this.baseURL}${endpoint}`;
    
    const headers: HeadersInit = {
      'Content-Type': 'application/json',
      ...options.headers,
    };

    if (this.token) {
      headers['Authorization'] = `Bearer ${this.token}`;
    }

    const response = await fetch(url, {
      ...options,
      headers,
    });

    if (!response.ok) {
      const error: ErrorResponse = await response.json();
      throw new Error(error.error.message);
    }

    return response.json();
  }

  // Authentication
  async registerParent(data: ParentRegistrationRequest): Promise<ParentRegistrationResponse> {
    return this.request('/api/v1/auth/register/parent', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async login(data: LoginRequest): Promise<LoginResponse> {
    return this.request('/api/v1/auth/login', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  // User Management
  async setPreferences(data: PreferencesRequest): Promise<PreferencesResponse> {
    return this.request('/api/v1/users/parent/preferences', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async createChild(data: ChildProfileRequest): Promise<ChildProfileResponse> {
    return this.request('/api/v1/users/child', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async selectExam(childId: string, data: ExamSelectionRequest): Promise<ExamSelectionResponse> {
    return this.request(`/api/v1/users/child/${childId}/exam`, {
      method: 'PUT',
      body: JSON.stringify(data),
    });
  }

  // Diagnostic Test
  async generateDiagnosticTest(data: DiagnosticGenerateRequest): Promise<DiagnosticGenerateResponse> {
    return this.request('/api/v1/diagnostic/generate', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async submitAnswer(data: SubmitAnswerRequest): Promise<SubmitAnswerResponse> {
    return this.request('/api/v1/diagnostic/submit-answer', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async submitTest(data: SubmitTestRequest): Promise<SubmitTestResponse> {
    return this.request('/api/v1/diagnostic/submit-test', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  // Analytics
  async getAnalytics(analyticsId: string): Promise<AnalyticsResponse> {
    return this.request(`/api/v1/analytics/${analyticsId}`);
  }

  // Schedule
  async generateSchedule(data: ScheduleGenerateRequest): Promise<ScheduleGenerateResponse> {
    return this.request('/api/v1/schedule/generate', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async getSchedule(scheduleId: string): Promise<ScheduleResponse> {
    return this.request(`/api/v1/schedule/${scheduleId}`);
  }

  async reschedule(data: RescheduleRequest): Promise<RescheduleResponse> {
    return this.request('/api/v1/schedule/reschedule', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  // Questions
  async generateQuestions(data: QuestionGenerateRequest): Promise<QuestionGenerateResponse> {
    return this.request('/api/v1/questions/generate', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async startPracticeSession(data: PracticeSessionRequest): Promise<PracticeSessionResponse> {
    return this.request('/api/v1/questions/practice-session', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  // Teaching Resources
  async getTeachingNotes(topic: string, query: TeachingNotesQuery): Promise<TeachingNotesResponse> {
    const params = new URLSearchParams(query as any);
    return this.request(`/api/v1/teaching/notes/${topic}?${params}`);
  }

  async getTeachingMethodology(topic: string): Promise<TeachingMethodologyResponse> {
    return this.request(`/api/v1/teaching/methodology/${topic}`);
  }

  async getMindmap(topic: string, query: MindmapQuery): Promise<MindmapResponse> {
    const params = new URLSearchParams(query as any);
    return this.request(`/api/v1/teaching/mindmap/${topic}?${params}`);
  }

  async getAudioSummary(topic: string, query: AudioSummaryQuery): Promise<AudioSummaryResponse> {
    const params = new URLSearchParams(query as any);
    return this.request(`/api/v1/teaching/audio-summary/${topic}?${params}`);
  }

  // Payment
  async createPaymentOrder(data: PaymentOrderRequest): Promise<PaymentOrderResponse> {
    return this.request('/api/v1/payment/create-order', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async verifyPayment(data: PaymentVerifyRequest): Promise<PaymentVerifyResponse> {
    return this.request('/api/v1/payment/verify', {
      method: 'POST',
      body: JSON.stringify(data),
    });
  }

  async getPaymentHistory(parentId: string): Promise<PaymentHistoryResponse> {
    return this.request(`/api/v1/payment/history/${parentId}`);
  }

  // Progress
  async getProgress(studentId: string): Promise<ProgressResponse> {
    return this.request(`/api/v1/progress/${studentId}`);
  }

  // Dashboards
  async getParentDashboard(parentId: string): Promise<ParentDashboardResponse> {
    return this.request(`/api/v1/dashboard/parent/${parentId}`);
  }

  async getStudentDashboard(studentId: string): Promise<StudentDashboardResponse> {
    return this.request(`/api/v1/dashboard/student/${studentId}`);
  }
}

export const apiClient = new APIClient(API_BASE_URL);
```

### API Testing Strategy

#### Backend Testing (Tushar)

**Test with curl**:
```bash
# Test parent registration
curl -X POST http://localhost:8000/api/v1/auth/register/parent \
  -H "Content-Type: application/json" \
  -d '{"email": "test@example.com", "language": "en"}'

# Expected response
{
  "parent_id": "abc123",
  "verification_required": true,
  "message": "Verification email sent"
}
```

**Test with Python**:
```python
# test_api.py
import requests

def test_parent_registration():
    response = requests.post(
        'http://localhost:8000/api/v1/auth/register/parent',
        json={'email': 'test@example.com', 'language': 'en'}
    )
    assert response.status_code == 200
    data = response.json()
    assert 'parent_id' in data
    assert 'verification_required' in data
```

#### Frontend Testing (Vaishnavi)

**Test with mock API server**:
```javascript
// mock-api-server.js
const express = require('express');
const app = express();

app.use(express.json());

// Mock parent registration
app.post('/api/v1/auth/register/parent', (req, res) => {
  res.json({
    parent_id: 'mock-parent-123',
    verification_required: false,
    message: 'Mock registration successful'
  });
});

// Mock all other endpoints...

app.listen(8000, () => {
  console.log('Mock API server running on port 8000');
});
```

**Test with React**:
```typescript
// Test component with mock API
import { apiClient } from '@/lib/api-client';

// In development, point to mock server
if (process.env.NODE_ENV === 'development') {
  apiClient.baseURL = 'http://localhost:8000';
}

// Use in component
const handleRegister = async (data: ParentRegistrationRequest) => {
  try {
    const response = await apiClient.registerParent(data);
    console.log('Registration successful:', response);
  } catch (error) {
    console.error('Registration failed:', error);
  }
};
```

### API Documentation

Both developers should have access to:

1. **OpenAPI/Swagger Docs**: Auto-generated from FastAPI
   - URL: `http://localhost:8000/docs`
   - Interactive API testing
   - Request/response examples

2. **Type Definitions**: Shared TypeScript types
   - File: `shared/types.ts`
   - Used by both backend (Pydantic) and frontend (TypeScript)

3. **API Contract Tests**: Automated tests to verify contract
   - Test that backend matches frontend expectations
   - Run before integration

### Integration Verification Checklist

Before integrating:
- [ ] All endpoint URLs match exactly
- [ ] All request types match (POST, GET, PUT, DELETE)
- [ ] All request body structures match
- [ ] All response structures match
- [ ] All error responses match
- [ ] All query parameters match
- [ ] All path parameters match
- [ ] All status codes match
- [ ] TypeScript types generated from backend
- [ ] API client tested with mock server
- [ ] Backend tested with curl/Python
- [ ] OpenAPI docs reviewed by both developers

## Detailed Google Cloud AI Services Guide

### Google Cloud Vector Search (Vertex AI)

#### What is Vector Search?

Vector Search allows you to store and search through text content using semantic meaning rather than exact keyword matches. For Mentor AI, we use it to store exam syllabus content and retrieve relevant context when generating questions.

#### How It Works

1. **Text â†’ Embeddings**: Convert syllabus text into numerical vectors (arrays of numbers)
2. **Store in Index**: Save these vectors in a searchable database
3. **Query**: Convert a question topic into a vector and find similar syllabus content
4. **Use Context**: Feed retrieved content to LLM for accurate question generation

#### Step-by-Step Setup (Day 3)

**PROMPT: Setup Vector Search Infrastructure**

```
Create Python code to set up Google Vertex AI Vector Search for educational content.

CONTEXT:
- Project: Mentor AI EdTech Platform
- Purpose: Store JEE/NEET syllabus content as embeddings for RAG
- Model: textembedding-gecko@003 (768 dimensions)

GENERATE:

1. Initialize Vertex AI client:
```python
from google.cloud import aiplatform
from vertexai.language_models import TextEmbeddingModel

# Initialize
aiplatform.init(project="your-project-id", location="us-central1")
embedding_model = TextEmbeddingModel.from_pretrained("textembedding-gecko@003")
```

2. Function to chunk syllabus text:
```python
def chunk_syllabus(text: str, chunk_size: int = 500) -> list:
    """
    Break syllabus into 200-500 word chunks.
    
    Args:
        text: Full syllabus text
        chunk_size: Target words per chunk
    
    Returns:
        List of chunks with metadata
    """
    # Split by paragraphs or sections
    # Maintain context at boundaries
    # Add metadata: topic, subject, exam_type
```

3. Function to generate embeddings:
```python
def generate_embeddings(chunks: list) -> list:
    """
    Convert text chunks to vector embeddings.
    
    Args:
        chunks: List of text chunks
    
    Returns:
        List of embeddings with metadata
    """
    embeddings = []
    for chunk in chunks:
        # Call embedding model
        embedding = embedding_model.get_embeddings([chunk["text"]])[0].values
        embeddings.append({
            "id": chunk["id"],
            "embedding": embedding,  # 768-dimensional vector
            "metadata": chunk["metadata"]
        })
    return embeddings
```

4. Function to create Vector Search index:
```python
def create_vector_index(display_name: str):
    """
    Create a new Vector Search index.
    
    Returns:
        Index resource
    """
    index = aiplatform.MatchingEngineIndex.create_tree_ah_index(
        display_name=display_name,
        dimensions=768,  # Must match embedding model
        approximate_neighbors_count=10,
        distance_measure_type="DOT_PRODUCT_DISTANCE",
        leaf_node_embedding_count=500,
        leaf_nodes_to_search_percent=7,
    )
    return index
```

5. Function to deploy index:
```python
def deploy_index(index, endpoint_display_name: str):
    """
    Deploy index to an endpoint for querying.
    """
    # Create endpoint
    endpoint = aiplatform.MatchingEngineIndexEndpoint.create(
        display_name=endpoint_display_name,
        public_endpoint_enabled=True,
    )
    
    # Deploy index to endpoint
    endpoint.deploy_index(
        index=index,
        deployed_index_id="syllabus_deployed",
        min_replica_count=1,
        max_replica_count=10,
    )
    return endpoint
```

6. Function to query (search) the index:
```python
def query_syllabus(query_text: str, exam_type: str, top_k: int = 5):
    """
    Search for relevant syllabus content.
    
    Args:
        query_text: Topic or question to search for
        exam_type: Filter by exam (JEE_MAIN, NEET, etc.)
        top_k: Number of results to return
    
    Returns:
        List of relevant syllabus chunks
    """
    # Generate embedding for query
    query_embedding = embedding_model.get_embeddings([query_text])[0].values
    
    # Search index
    response = endpoint.find_neighbors(
        deployed_index_id="syllabus_deployed",
        queries=[query_embedding],
        num_neighbors=top_k,
    )
    
    # Filter by exam type
    results = [
        neighbor.metadata 
        for neighbor in response[0]
        if neighbor.metadata.get("exam_type") == exam_type
    ]
    
    return results
```

REQUIREMENTS:
- Add comprehensive error handling
- Include retry logic (3 attempts) for API calls
- Add logging for debugging
- Cache frequently accessed embeddings
- Include example usage
- Add type hints and docstrings

OUTPUT:
Complete Python file with all functions and example usage.
```

**What You'll Get:**
- Complete vector_search_service.py file
- Functions to chunk, embed, index, and query syllabus content
- Ready to use for RAG implementation

**Testing:**
```bash
# Test embedding generation
python -c "from vector_search_service import generate_embeddings; print(generate_embeddings([{'text': 'Newton laws of motion', 'id': '1', 'metadata': {}}]))"

# Test query
python -c "from vector_search_service import query_syllabus; print(query_syllabus('mechanics', 'JEE_MAIN'))"
```

### Retrieval-Augmented Generation (RAG)

#### What is RAG?

RAG combines retrieval (finding relevant information) with generation (creating new content). Instead of relying only on the LLM's training data, RAG retrieves current, specific information and provides it as context.

#### Why Use RAG for Mentor AI?

1. **Accuracy**: Questions are grounded in actual syllabus content
2. **Up-to-date**: Can update syllabus without retraining LLM
3. **Exam-specific**: Ensures questions match official exam patterns
4. **Reduces hallucination**: LLM has concrete facts to work with

#### RAG Flow for Question Generation

```
1. User requests: "Generate JEE Main mechanics questions"
2. Query Vector Search: Find relevant mechanics syllabus chunks
3. Retrieved Context: "Newton's laws, friction, circular motion..."
4. Prompt LLM: "Using this syllabus content, generate 10 questions..."
5. LLM generates: Questions based on actual syllabus
```

#### Implementation (Day 3)

**PROMPT: Implement RAG for Question Generation**

```
Create Python code to generate exam questions using RAG with Vector Search.

CONTEXT:
- Project: Mentor AI EdTech Platform
- Purpose: Generate accurate, syllabus-aligned questions
- Uses: Vector Search for retrieval, Gemini for generation

GENERATE:

1. Function to retrieve syllabus context:
```python
def get_syllabus_context(topic: str, exam_type: str) -> str:
    """
    Retrieve relevant syllabus content for a topic.
    
    Args:
        topic: Topic name (e.g., "Mechanics", "Organic Chemistry")
        exam_type: Exam type (JEE_MAIN, NEET, etc.)
    
    Returns:
        Concatenated syllabus text
    """
    # Query vector search
    results = query_syllabus(topic, exam_type, top_k=5)
    
    # Combine retrieved chunks
    context = "\n\n".join([
        f"Topic: {r['topic']}\nContent: {r['text']}"
        for r in results
    ])
    
    return context
```

2. Function to generate questions with RAG:
```python
from vertexai.generative_models import GenerativeModel

def generate_questions_with_rag(
    topic: str,
    exam_type: str,
    difficulty: str,
    count: int = 10
) -> list:
    """
    Generate questions using RAG.
    
    Args:
        topic: Topic name
        exam_type: Exam type
        difficulty: easy, medium, or hard
        count: Number of questions
    
    Returns:
        List of generated questions
    """
    # Step 1: Retrieve syllabus context
    context = get_syllabus_context(topic, exam_type)
    
    # Step 2: Create prompt with context
    prompt = f"""
You are an expert {exam_type} question generator.

SYLLABUS CONTEXT:
{context}

TASK:
Generate {count} {difficulty} level multiple-choice questions on {topic}.

REQUIREMENTS:
1. Questions must be based ONLY on the syllabus context provided above
2. Follow {exam_type} exam pattern exactly
3. Include 4 options (A, B, C, D) with one correct answer
4. Provide detailed explanation for correct answer
5. Match difficulty level: {difficulty}

OUTPUT FORMAT (JSON):
[
  {{
    "question": "Question text",
    "options": {{
      "A": "Option A",
      "B": "Option B",
      "C": "Option C",
      "D": "Option D"
    }},
    "correct_answer": "A",
    "explanation": "Step-by-step explanation",
    "difficulty": "{difficulty}",
    "topic": "{topic}"
  }}
]
"""
    
    # Step 3: Call Gemini to generate
    model = GenerativeModel("gemini-1.5-flash")
    response = model.generate_content(prompt)
    
    # Step 4: Parse response
    questions = parse_json_response(response.text)
    
    return questions
```

3. Helper function to parse LLM response:
```python
import json
import re

def parse_json_response(text: str) -> list:
    """
    Extract JSON from LLM response.
    
    Args:
        text: Raw LLM response
    
    Returns:
        Parsed list of questions
    """
    # Find JSON in response (may have markdown code blocks)
    json_match = re.search(r'```json\n(.*?)\n```', text, re.DOTALL)
    if json_match:
        json_text = json_match.group(1)
    else:
        json_text = text
    
    # Parse JSON
    try:
        questions = json.loads(json_text)
        return questions
    except json.JSONDecodeError as e:
        # Log error and return empty list
        print(f"Failed to parse JSON: {e}")
        return []
```

REQUIREMENTS:
- Validate retrieved context is not empty
- Handle JSON parsing errors gracefully
- Add retry logic for LLM API calls
- Log all prompts and responses for debugging
- Validate generated questions match requirements
- Include comprehensive error handling

OUTPUT:
Complete Python file with RAG implementation and example usage.
```

**What You'll Get:**
- Complete question_generation_service.py file
- RAG implementation combining Vector Search + Gemini
- Question validation and parsing logic

**Testing:**
```bash
# Test RAG question generation
python -c "from question_generation_service import generate_questions_with_rag; print(generate_questions_with_rag('Mechanics', 'JEE_MAIN', 'medium', 5))"
```

### Google Gemini API

#### What is Gemini?

Gemini is Google's large language model (LLM) for generating text, analyzing content, and reasoning. For Mentor AI, we use Gemini Flash (fast, cost-effective) for:
- Analyzing diagnostic test results
- Generating study schedules
- Creating teaching resources
- Generating questions (with RAG)

#### Gemini Flash vs Gemini Pro

- **Gemini Flash**: Faster, cheaper, good for structured tasks
- **Gemini Pro**: More powerful, better for complex reasoning
- **For Mentor AI**: Use Flash for most tasks (sufficient quality, lower cost)

#### Key Concepts

1. **Prompt Engineering**: How you ask determines what you get
2. **Temperature**: Controls randomness (0 = deterministic, 1 = creative)
3. **Tokens**: Units of text (roughly 4 characters = 1 token)
4. **Context Window**: Maximum input size (Gemini Flash: 1M tokens)

#### Implementation (Day 4 & 5)

**PROMPT: Implement Gemini Flash for Analytics**

```
Create Python code to analyze diagnostic test results using Gemini Flash.

CONTEXT:
- Project: Mentor AI EdTech Platform
- Purpose: Generate detailed analytics and improvement strategies
- Model: gemini-1.5-flash

GENERATE:

1. Initialize Gemini:
```python
from vertexai.generative_models import GenerativeModel, GenerationConfig

# Initialize model
model = GenerativeModel("gemini-1.5-flash")

# Configure generation
config = GenerationConfig(
    temperature=0.2,  # Low temperature for consistent analysis
    top_p=0.8,
    top_k=40,
    max_output_tokens=2048,
)
```

2. Function to analyze diagnostic test:
```python
def analyze_diagnostic_test(
    test_results: dict,
    topic_weightages: dict,
    exam_type: str
) -> dict:
    """
    Analyze test results and generate insights.
    
    Args:
        test_results: {
            "overall_score": 65,
            "subject_scores": [
                {"subject": "Physics", "score": 55, "total": 100},
                {"subject": "Chemistry", "score": 70, "total": 100},
                {"subject": "Math", "score": 70, "total": 100}
            ],
            "topic_scores": [
                {"topic": "Mechanics", "score": 45, "weightage": 15},
                {"topic": "Thermodynamics", "score": 60, "weightage": 10},
                ...
            ]
        }
        topic_weightages: {"Mechanics": 15, "Thermodynamics": 10, ...}
        exam_type: "JEE_MAIN", "NEET", etc.
    
    Returns:
        Analytics with weak areas and strategies
    """
    
    # Create detailed prompt
    prompt = f"""
You are an expert {exam_type} exam analyst.

STUDENT PERFORMANCE:
Overall Score: {test_results['overall_score']}%

Subject-wise Performance:
{format_subject_scores(test_results['subject_scores'])}

Topic-wise Performance (with exam weightage):
{format_topic_scores(test_results['topic_scores'], topic_weightages)}

TASK:
Analyze this diagnostic test and provide:

1. WEAK SUBJECTS (score < 60%):
   - List each weak subject
   - Explain why performance is low
   - Identify specific weak topics within each subject

2. WEAK TOPICS (score < 60%):
   - List all weak topics
   - Include their exam weightage percentage
   - Prioritize high-weightage topics

3. IMPROVEMENT STRATEGIES:
   - For each weak topic, provide:
     * Specific focus areas
     * Study techniques
     * Practice priorities
     * Estimated time to improve (based on weightage and difficulty)
   - Prioritize strategies for high-weightage weak topics

4. PARENT GUIDANCE:
   - How parents can help with each weak area
   - Teaching approaches for difficult topics
   - Monitoring and support strategies

OUTPUT FORMAT (JSON):
{{
  "weak_subjects": [
    {{
      "subject": "Physics",
      "score": 55,
      "reason": "Weak in mechanics and thermodynamics",
      "weak_topics": ["Mechanics", "Thermodynamics"]
    }}
  ],
  "weak_topics": [
    {{
      "topic": "Mechanics",
      "score": 45,
      "weightage": 15,
      "priority": "HIGH",
      "reason": "High weightage topic with low score"
    }}
  ],
  "strategies": [
    {{
      "topic": "Mechanics",
      "weightage": 15,
      "focus_areas": ["Newton's laws", "Friction", "Circular motion"],
      "study_techniques": ["Solve numerical problems", "Draw free body diagrams"],
      "estimated_time": "2 weeks of focused practice",
      "parent_guidance": "Help with problem-solving approach, review solutions together"
    }}
  ]
}}
"""
    
    # Call Gemini
    response = model.generate_content(
        prompt,
        generation_config=config
    )
    
    # Parse response
    analytics = parse_json_response(response.text)
    
    return analytics
```

3. Helper functions:
```python
def format_subject_scores(scores: list) -> str:
    """Format subject scores for prompt."""
    return "\n".join([
        f"- {s['subject']}: {s['score']}/{s['total']} ({s['score']/s['total']*100:.1f}%)"
        for s in scores
    ])

def format_topic_scores(scores: list, weightages: dict) -> str:
    """Format topic scores with weightages."""
    return "\n".join([
        f"- {s['topic']}: {s['score']}% (Exam weightage: {weightages.get(s['topic'], 0)}%)"
        for s in scores
    ])
```

4. Retry logic for API calls:
```python
import time

def call_gemini_with_retry(prompt: str, max_retries: int = 3):
    """
    Call Gemini with exponential backoff retry.
    """
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt, generation_config=config)
            return response
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            wait_time = 2 ** attempt  # Exponential backoff
            print(f"Retry {attempt + 1}/{max_retries} after {wait_time}s...")
            time.sleep(wait_time)
```

REQUIREMENTS:
- Use temperature=0.2 for consistent analysis
- Include retry logic with exponential backoff
- Validate JSON response format
- Log all prompts and responses
- Handle API errors gracefully
- Add comprehensive error handling

OUTPUT:
Complete Python file with Gemini analytics implementation.
```

**What You'll Get:**
- Complete analytics_service.py file
- Gemini Flash integration for test analysis
- Retry logic and error handling

**Testing:**
```bash
# Test analytics generation
python -c "from analytics_service import analyze_diagnostic_test; print(analyze_diagnostic_test(sample_test_results, sample_weightages, 'JEE_MAIN'))"
```

**PROMPT: Implement Gemini Flash for Schedule Generation**

```
Create Python code to generate study schedules using Gemini Flash.

CONTEXT:
- Project: Mentor AI EdTech Platform
- Purpose: Create personalized day-by-day study plans
- Model: gemini-1.5-flash

GENERATE:

1. Function to generate schedule:
```python
def generate_study_schedule(
    analytics: dict,
    exam_date: str,
    days_remaining: int,
    preferences: dict
) -> dict:
    """
    Generate personalized study schedule.
    
    Args:
        analytics: Output from analyze_diagnostic_test()
        exam_date: Target exam date (YYYY-MM-DD)
        days_remaining: Days until exam
        preferences: {
            "study_hours_per_day": 6,
            "preferred_times": ["morning", "evening"],
            "current_prep_status": "self_studying"
        }
    
    Returns:
        Day-by-day schedule with tasks
    """
    
    prompt = f"""
You are an expert {exam_type} study planner.

STUDENT SITUATION:
- Exam Date: {exam_date}
- Days Remaining: {days_remaining}
- Daily Study Hours Available: {preferences['study_hours_per_day']}
- Current Status: {preferences['current_prep_status']}

WEAK AREAS (from diagnostic test):
{format_weak_topics(analytics['weak_topics'])}

TOPIC WEIGHTAGES (exam importance):
{format_weightages(analytics['weak_topics'])}

TASK:
Create a day-by-day study schedule that:

1. PRIORITIZES high-weightage weak topics
2. ALLOCATES time proportional to weightage and weakness
3. INCLUDES:
   - Daily topic coverage
   - Practice sessions
   - Review sessions
   - Weekly mock tests
4. MAINTAINS realistic daily hours ({preferences['study_hours_per_day']} hours/day)
5. BUILDS progressively from basics to advanced

OUTPUT FORMAT (JSON):
{{
  "schedule_id": "unique_id",
  "generated_at": "2024-01-01T00:00:00Z",
  "daily_tasks": [
    {{
      "day": 1,
      "date": "2024-01-01",
      "topics": ["Mechanics - Newton's Laws"],
      "activities": [
        {{
          "type": "CONCEPT_LEARNING",
          "topic": "Newton's Laws",
          "duration_hours": 2,
          "weightage": 15,
          "description": "Study theory and examples"
        }},
        {{
          "type": "PRACTICE",
          "topic": "Newton's Laws",
          "duration_hours": 2,
          "description": "Solve 20 numerical problems"
        }},
        {{
          "type": "REVIEW",
          "topic": "Previous weak topics",
          "duration_hours": 1,
          "description": "Quick revision"
        }}
      ],
      "total_hours": 5,
      "milestone": null
    }},
    ...
  ],
  "milestones": [
    {{
      "week": 1,
      "goal": "Complete Mechanics basics",
      "topics_covered": ["Newton's Laws", "Friction"],
      "assessment": "Mock test on Mechanics"
    }}
  ]
}}
"""
    
    response = call_gemini_with_retry(prompt)
    schedule = parse_json_response(response.text)
    
    return schedule
```

2. Function for adaptive rescheduling:
```python
def regenerate_schedule(
    schedule_id: str,
    reason: str,
    current_progress: dict
) -> dict:
    """
    Regenerate schedule when student falls behind or improves.
    
    Args:
        schedule_id: Existing schedule ID
        reason: "MISSED_SESSION" or "PRACTICE_TEST_PERFORMANCE"
        current_progress: {
            "completed_topics": ["Newton's Laws"],
            "missed_sessions": 3,
            "new_weak_topics": ["Thermodynamics"],
            "improved_topics": ["Organic Chemistry"]
        }
    
    Returns:
        Updated schedule
    """
    
    prompt = f"""
You are adapting a study schedule based on student progress.

SITUATION:
- Reason for rescheduling: {reason}
- Completed topics: {current_progress['completed_topics']}
- Missed sessions: {current_progress['missed_sessions']}
- New weak topics: {current_progress.get('new_weak_topics', [])}
- Improved topics: {current_progress.get('improved_topics', [])}

TASK:
Regenerate the remaining schedule to:

1. REDISTRIBUTE pending topics across remaining days
2. PRIORITIZE newly identified weak topics (especially high-weightage)
3. REDUCE time for improved topics
4. MAINTAIN realistic daily hours
5. ENSURE all critical topics are covered before exam

OUTPUT FORMAT: Same as generate_study_schedule()
"""
    
    response = call_gemini_with_retry(prompt)
    updated_schedule = parse_json_response(response.text)
    
    return updated_schedule
```

REQUIREMENTS:
- Validate days_remaining > 0
- Ensure total daily hours <= study_hours_per_day
- Include error handling for invalid dates
- Log all schedule generations
- Add retry logic for API calls

OUTPUT:
Complete Python file with schedule generation.
```

**What You'll Get:**
- Complete schedule_service.py file
- Schedule generation and adaptive rescheduling
- Gemini Flash integration

**Testing:**
```bash
# Test schedule generation
python -c "from schedule_service import generate_study_schedule; print(generate_study_schedule(sample_analytics, '2024-06-01', 90, sample_preferences))"
```

### Common Gemini API Issues and Solutions

| Issue | Cause | Solution |
|-------|-------|----------|
| Rate limit exceeded | Too many requests | Add retry with exponential backoff |
| Invalid JSON response | LLM didn't format correctly | Use regex to extract JSON, add validation |
| Empty response | Prompt too vague | Make prompt more specific with examples |
| Timeout | Large context | Reduce input size or increase timeout |
| API key error | Missing/invalid key | Check GOOGLE_AI_API_KEY environment variable |

### Best Practices for Gemini Prompts

1. **Be Specific**: "Generate 10 JEE Main mechanics questions" not "Generate questions"
2. **Provide Context**: Include syllabus content, exam pattern, difficulty level
3. **Request Format**: Always specify JSON output format with example
4. **Set Temperature**: Use 0.2-0.4 for consistent, factual outputs
5. **Add Constraints**: "Questions must be based ONLY on provided syllabus"
6. **Include Examples**: Show desired output format in prompt
7. **Validate Output**: Always parse and validate JSON responses

## Maintenance and Updates

### When to Update Tutorial

1. **LLM Model Changes**
   - Update prompts if model behavior changes
   - Test all prompts with new model versions

2. **Dependency Updates**
   - Update version numbers in prompts
   - Test compatibility

3. **API Changes**
   - Update Firebase/Google Cloud instructions
   - Update Razorpay integration steps

4. **Bug Fixes**
   - Document common issues in appendix
   - Update troubleshooting sections

### Version Control

- Tutorial version: 1.0.0
- Compatible with:
  - Python 3.11+
  - Node.js 18+
  - FastAPI 0.104+
  - Next.js 14+
  - Firebase Admin SDK 6.2+
  - Google Cloud AI Platform 1.38+
